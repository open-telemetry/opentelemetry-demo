---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  namespace: opentelemetry-demo
  labels:
    helm.sh/chart: opentelemetry-collector-0.142.2
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: otel-demo
    app.kubernetes.io/version: "0.142.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: opentelemetry-collector
    app.kubernetes.io/component: standalone-collector
---
# Source: opentelemetry-demo/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-demo
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector
  namespace: opentelemetry-demo
  labels:
    helm.sh/chart: opentelemetry-collector-0.142.2
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: otel-demo
    app.kubernetes.io/version: "0.142.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: opentelemetry-collector
    app.kubernetes.io/component: standalone-collector
data:
  relay: |
    exporters:
      debug: {}
      otlphttp/newrelic:
        endpoint: https://otlp.nr-data.net:4318
        headers:
          api-key: ${env:NR_LICENSE_KEY}
    extensions:
      health_check:
        endpoint: 0.0.0.0:13133
      k8s_observer:
        auth_type: serviceAccount
        node: ${env:K8S_NODE_NAME}
    processors:
      batch: {}
      cumulativetodelta: {}
      k8sattributes:
        extract:
          metadata:
          - k8s.namespace.name
          - k8s.deployment.name
          - k8s.statefulset.name
          - k8s.daemonset.name
          - k8s.cronjob.name
          - k8s.job.name
          - k8s.node.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.pod.start_time
          otel_annotations: true
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: connection
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_percentage: 25
      resource:
        attributes:
        - action: insert
          from_attribute: k8s.pod.name
          key: service.instance.id
      resourcedetection/env:
        detectors:
        - env
        - system
        override: false
        system:
          hostname_sources:
          - os
          resource_attributes:
            host.id:
              enabled: true
      transform:
        error_mode: ignore
        log_statements:
        - context: log
          statements:
          - truncate_all(attributes, 4095)
          - truncate_all(resource.attributes, 4095)
        trace_statements:
        - context: span
          statements:
          - truncate_all(attributes, 4095)
          - truncate_all(resource.attributes, 4095)
        - context: span
          statements:
          - replace_pattern(name, "\\?.*", "")
          - replace_match(name, "GET /api/products/*", "GET /api/products/{productId}")
    receivers:
      hostmetrics:
        collection_interval: 10s
        root_path: /hostfs
        scrapers:
          cpu: null
          disk: null
          filesystem:
            exclude_fs_types:
              fs_types:
              - autofs
              - binfmt_misc
              - bpf
              - cgroup2
              - configfs
              - debugfs
              - devpts
              - devtmpfs
              - fusectl
              - hugetlbfs
              - iso9660
              - mqueue
              - nsfs
              - overlay
              - proc
              - procfs
              - pstore
              - rpc_pipefs
              - securityfs
              - selinuxfs
              - squashfs
              - sysfs
              - tracefs
              match_type: strict
            exclude_mount_points:
              match_type: regexp
              mount_points:
              - /dev/*
              - /proc/*
              - /sys/*
              - /run/k3s/containerd/*
              - /var/lib/docker/*
              - /var/lib/kubelet/*
              - /snap/*
          load: null
          memory: null
          network: null
      httpcheck/frontend-proxy:
        targets:
        - endpoint: http://frontend-proxy:8080
      k8s_cluster:
        collection_interval: 10s
      kubeletstats:
        auth_type: serviceAccount
        collection_interval: 20s
        endpoint: ${env:K8S_NODE_IP}:10250
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            cors:
              allowed_origins:
              - http://*
              - https://*
            endpoint: ${env:MY_POD_IP}:4318
      prometheus:
        config:
          scrape_configs:
          - job_name: opentelemetry-collector
            scrape_interval: 30s
            static_configs:
            - targets:
              - ${env:MY_POD_IP}:8888
      receiver_creator/metrics:
        discovery:
          enabled: true
        watch_observers:
        - k8s_observer
      redis:
        collection_interval: 30s
        endpoint: valkey-cart:6379
    service:
      extensions:
      - health_check
      - k8s_observer
      pipelines:
        logs:
          exporters:
          - otlphttp/newrelic
          - debug
          processors:
          - k8sattributes
          - memory_limiter
          - resource
          - transform
          - batch
          receivers:
          - otlp
        metrics:
          exporters:
          - otlphttp/newrelic
          - debug
          processors:
          - memory_limiter
          - resource
          - resourcedetection/env
          - k8sattributes
          - cumulativetodelta
          - batch
          receivers:
          - hostmetrics
          - redis
          - otlp
          - receiver_creator/metrics
          - kubeletstats
          - k8s_cluster
        traces:
          exporters:
          - otlphttp/newrelic
          - debug
          processors:
          - k8sattributes
          - memory_limiter
          - resource
          - resourcedetection/env
          - transform
          - batch
          receivers:
          - otlp
      telemetry:
        metrics:
          level: basic
          readers:
          - periodic:
              exporter:
                otlp:
                  endpoint: http://${env:MY_POD_IP}:4318
                  protocol: http/protobuf
              interval: 30000
              timeout: 5000
---
# Source: opentelemetry-demo/templates/flagd-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: flagd-config
  namespace: opentelemetry-demo
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
data:
  
  demo.flagd.json: |
    {
      "$schema": "https://flagd.dev/schema/v0/flags.json",
      "flags": {
        "llmInaccurateResponse": {
          "defaultVariant": "off",
          "description": "LLM returns an inaccurate product summary for product ID L9ECAV7KIM",
          "state": "ENABLED",
          "variants": {
            "off": false,
            "on": true
          }
        },
        "llmRateLimitError": {
          "defaultVariant": "off",
          "description": "LLM intermittently returns a rate limit error",
          "state": "ENABLED",
          "variants": {
            "off": false,
            "on": true
          }
        },
        "productCatalogFailure": {
          "description": "Fail product catalog service on a specific product",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "recommendationCacheFailure": {
          "description": "Fail recommendation service cache",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "adManualGc": {
          "description": "Triggers full manual garbage collections in the ad service",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "adHighCpu": {
          "description": "Triggers high cpu load in the ad service",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "adFailure": {
          "description": "Fail ad service",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "kafkaQueueProblems": {
          "description": "Overloads Kafka queue while simultaneously introducing a consumer side delay leading to a lag spike",
          "state": "ENABLED",
          "variants": {
            "on": 100,
            "off": 0
          },
          "defaultVariant": "off"
        },
        "cartFailure": {
          "description": "Fail cart service",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "paymentFailure": {
          "description": "Fail payment service charge requests n%",
          "state": "ENABLED",
          "variants": {
            "100%": 1,
            "90%": 0.95,
            "75%": 0.75,
            "50%": 0.5,
            "25%": 0.25,
            "10%": 0.1,
            "off": 0
          },
          "defaultVariant": "off"
        },
        "paymentUnreachable": {
          "description": "Payment service is unavailable",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "loadGeneratorFloodHomepage": {
          "description": "Flood the frontend with a large amount of requests.",
          "state": "ENABLED",
          "variants": {
            "on": 100,
            "off": 0
          },
          "defaultVariant": "off"
        },
        "imageSlowLoad": {
          "description": "slow loading images in the frontend",
          "state": "ENABLED",
          "variants": {
            "10sec": 10000,
            "5sec": 5000,
            "off": 0
          },
          "defaultVariant": "off"
        },
        "failedReadinessProbe": {
          "description": "readiness probe failure for cart service",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "emailMemoryLeak": {
          "description": "Memory leak in the email service.",
          "state": "ENABLED",
          "variants": {
            "off": 0,
            "1x": 1,
            "10x": 10,
            "100x": 100,
            "1000x": 1000,
            "10000x": 10000
          },
          "defaultVariant": "off"
        }
      }
    }
---
# Source: opentelemetry-demo/templates/posgresql-init-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgresql-init
  namespace: opentelemetry-demo
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
data:
  
  init.sql: |
    -- Copyright The OpenTelemetry Authors
    -- SPDX-License-Identifier: Apache-2.0
  
    CREATE USER otelu WITH PASSWORD 'otelp';
  
    -- Accounting Service: create a schema
    CREATE SCHEMA accounting;
    GRANT USAGE ON SCHEMA accounting TO otelu;
  
    -- Accounting Service: create tables
    CREATE TABLE accounting."order" (
        order_id TEXT PRIMARY KEY
    );
  
    CREATE TABLE accounting.shipping (
        shipping_tracking_id TEXT PRIMARY KEY,
        shipping_cost_currency_code TEXT NOT NULL,
        shipping_cost_units BIGINT NOT NULL,
        shipping_cost_nanos INT NOT NULL,
        street_address TEXT,
        city TEXT,
        state TEXT,
        country TEXT,
        zip_code TEXT,
        order_id TEXT NOT NULL,
        FOREIGN KEY (order_id) REFERENCES accounting."order"(order_id) ON DELETE CASCADE
    );
  
    CREATE TABLE accounting.orderitem (
        item_cost_currency_code TEXT NOT NULL,
        item_cost_units BIGINT NOT NULL,
        item_cost_nanos INT NOT NULL,
        product_id TEXT NOT NULL,
        quantity INT NOT NULL,
        order_id TEXT NOT NULL,
        PRIMARY KEY (order_id, product_id),
        FOREIGN KEY (order_id) REFERENCES accounting."order"(order_id) ON DELETE CASCADE
    );
  
    -- Accounting Service: grant permission to schema
    GRANT SELECT, INSERT, UPDATE ON ALL TABLES IN SCHEMA accounting TO otelu;
  
    -- Product Review Service: create a schema
    CREATE SCHEMA reviews;
    GRANT USAGE ON SCHEMA reviews TO otelu;
  
    -- Product Review Service: create tables
    CREATE TABLE reviews.productreviews (
        id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
        product_id VARCHAR(16) NOT NULL,
        username VARCHAR(64) NOT NULL,
        description VARCHAR(1024),
        score NUMERIC(2,1) NOT NULL
    );
  
    -- Product Review Service: create index for product_id lookups
    CREATE INDEX product_id_index ON reviews.productreviews (product_id);
  
    -- Product Review Service: grant permission to schema
    GRANT SELECT, INSERT, UPDATE ON ALL TABLES IN SCHEMA reviews TO otelu;
  
    -- Product Review Service:  add product review data
    INSERT INTO reviews.productreviews (product_id, username, description, score)
    VALUES
        ('OLJCESPC7Z', 'stargazer_mike', 'Great entry-level telescope! Easy to set up and provides clear views of the moon and brighter planets. Highly recommend for new astronomers.', '4.5'),
        ('OLJCESPC7Z', 'nightskylover', 'For the price, this Explorascope delivers excellent performance. I was able to see Jupiter''s moons clearly. A fantastic purchase for casual viewing.', '4.0'),
        ('OLJCESPC7Z', 'beginner_astro', 'A bit tricky to get used to the manual controls, but once you do, it''s very rewarding. Saw the Orion Nebula for the first time! Good value.', '3.5'),
        ('OLJCESPC7Z', 'celestial_explorer', 'Perfect for camping trips. It''s lightweight and portable, making it easy to take anywhere. The views are surprisingly good for its size.', '4.0'),
        ('OLJCESPC7Z', 'telescope_fan', 'Not the most powerful scope, but it''s great for kids and beginners. My children love looking at the moon with it. A solid choice for family fun.', '3.0'),
  
        ('66VCHSJNUP', 'tech_astro', 'The StarSense app is revolutionary! It made finding celestial objects incredibly easy. This telescope is a game-changer for beginners.', '5.0'),
        ('66VCHSJNUP', 'app_user', 'Amazing technology, the smartphone integration works flawlessly. I''ve never had so much fun exploring the night sky. Worth every penny.', '4.5'),
        ('66VCHSJNUP', 'innovator_john', 'Setup was a breeze, and the tutorials in the app are very helpful. The views are crisp and clear. My only minor gripe is battery drain on the phone.', '4.0'),
        ('66VCHSJNUP', 'clear_skies', 'Finally, a telescope that takes the guesswork out of stargazing. The real-time positioning is incredibly accurate. Highly recommended for anyone new to astronomy.', '5.0'),
        ('66VCHSJNUP', 'gadget_geek', 'Fantastic product, the app truly guides you. It''s like having a personal astronomer with you. The optical quality is also very good.', '4.5'),
  
        ('1YMWWN1N4O', 'solar_viewer', 'Perfect for solar observations! The Solar Safe filter gives peace of mind. I used it for the last partial eclipse and it was fantastic.', '5.0'),
        ('1YMWWN1N4O', 'eclipse_chaser', 'Compact and easy to carry, this telescope is ideal for eclipse events. The included backpack is a nice touch. Views of the sun are incredibly clear and safe.', '4.5'),
        ('1YMWWN1N4O', 'travel_astro', 'Excellent travel scope for solar viewing. The magnification is much better than binoculars for the sun. A must-have for any solar enthusiast.', '4.0'),
        ('1YMWWN1N4O', 'sun_gazer', 'Very impressed with the safety features and clarity. Sharing the sun with family has never been easier or safer. Great value for a dedicated solar scope.', '5.0'),
        ('1YMWWN1N4O', 'safe_viewer', 'The ISO compliant filter is reassuring. It''s a well-designed product for safe solar observation. Highly recommend for educational purposes too.', '4.5'),
  
        ('L9ECAV7KIM', 'clean_optics', 'This kit is a lifesaver for all my optics. The brush and wipes work perfectly without leaving any residue. My lenses have never been cleaner.', '5.0'),
        ('L9ECAV7KIM', 'photog_pro', 'Essential for any photographer or telescope owner. It safely removes dust and fingerprints. A high-quality cleaning solution.', '4.5'),
        ('L9ECAV7KIM', 'daily_cleaner', 'I use this on my binoculars, camera lenses, and even my phone screen. It''s very effective and gentle. A versatile cleaning kit.', '4.0'),
        ('L9ECAV7KIM', 'tech_maintenance', 'Great value for money. The different cleaning options cover all needs. Keeps my expensive equipment in pristine condition.', '5.0'),
        ('L9ECAV7KIM', 'sharp_view', 'Works as advertised, my telescope views are much clearer after using this. The fluid and cloth are excellent. Definitely recommend.', '4.5'),
  
        ('2ZYFJ3GM2N', 'bird_watcher', 'Incredible clarity and brightness, perfect for bird watching. The ED glass really makes a difference. I can spot the subtlest markings.', '5.0'),
        ('2ZYFJ3GM2N', 'nature_lover', 'These binoculars are fantastic for nature observation. The close focus is a huge advantage for viewing nearby wildlife. Very comfortable to hold.', '4.5'),
        ('2ZYFJ3GM2N', 'hiker_guy', 'Lightweight and durable, these are my go-to binoculars for hiking. The wide field of view is excellent. Highly recommend for outdoor enthusiasts.', '4.0'),
        ('2ZYFJ3GM2N', 'stadium_fan', 'Took these to a game and had an amazing view of the action. They perform great in various lighting conditions. A solid all-around binocular.', '4.0'),
        ('2ZYFJ3GM2N', 'outdoor_adventurer', 'Excellent build quality and optical performance. They feel robust and provide sharp images. A great investment for any outdoor activity.', '4.5'),
  
        ('0PUK6V6EV0', 'astro_photog', 'This imager is a fantastic step up for planetary photography. The color quality is superb. Easy to use with my existing telescope setup.', '5.0'),
        ('0PUK6V6EV0', 'planet_shooter', 'Finally capturing stunning images of Saturn and Jupiter! The NexImage 10 makes it so accessible. Great for beginners in astrophotography.', '4.5'),
        ('0PUK6V6EV0', 'imager_pro', 'Excellent resolution and color rendition for its price point. It''s a perfect solution for those looking to start imaging planets. Highly satisfied.', '4.0'),
        ('0PUK6V6EV0', 'space_artist', 'The detail I can capture with this imager is incredible. It integrates well with various software. A must-have for serious planetary observers.', '5.0'),
        ('0PUK6V6EV0', 'digital_sky', 'A solid choice for getting into solar system imaging. The setup was straightforward. Produces beautiful, vibrant planetary images.', '4.5'),
  
        ('LS4PSXUNUM', 'night_walker', 'The red light is perfect for preserving night vision during astronomy sessions. The hand warmer is an unexpected bonus. Very practical device.', '5.0'),
        ('LS4PSXUNUM', 'star_party_goer', 'This flashlight is indispensable for star parties. The red mode is gentle on the eyes, and the power bank feature is super handy. Love it!', '4.5'),
        ('LS4PSXUNUM', 'camper_chris', 'Rugged and versatile, this flashlight is great for camping and night walks. The hand warmer function is a game-changer on cold nights. Highly recommend.', '4.5'),
        ('LS4PSXUNUM', 'emergency_kit', 'A fantastic multi-tool for my emergency kit. The red light is useful, and the power bank means I can charge my phone. Great design.', '4.0'),
        ('LS4PSXUNUM', 'astro_accessory', 'Every astronomer needs one of these. The red light is essential, and the hand warmer and power bank make it incredibly useful. A top-tier accessory.', '5.0'),
  
        ('9SIQT8TOJO', 'deep_sky_master', 'The RASA V2 is a dream come true for deep-sky imaging. The f/2.2 speed drastically cuts down exposure times. My best astrophotography investment yet.', '5.0'),
        ('9SIQT8TOJO', 'pro_astro', 'Unbelievable performance for wide-field astrophotography. The short focal length makes guiding less critical. Produces stunning, detailed images.', '5.0'),
        ('9SIQT8TOJO', 'imaging_guru', 'This OTA is a beast! The fast optics mean more data in less time. If you''re serious about deep-sky imaging, this is the one.', '4.5'),
        ('9SIQT8TOJO', 'advanced_scope', 'Worth every penny for the quality and speed it offers. My images have never been sharper or more vibrant. A truly professional piece of equipment.', '5.0'),
        ('9SIQT8TOJO', 'precision_optics', 'The engineering behind this RASA is exceptional. It''s incredibly efficient for capturing faint objects. A high-end choice for dedicated imagers.', '4.5'),
  
        ('6E92ZMYYFZ', 'solar_safety', 'Essential for safe solar viewing with my 8-inch telescope. The Velcro straps ensure it stays securely in place. Peace of mind during solar observations.', '5.0'),
        ('6E92ZMYYFZ', 'telescope_upgrade', 'This EclipSmart filter is a perfect addition to my setup. The ISO compliance is crucial. Highly recommend for anyone looking to view the sun safely.', '4.5'),
        ('6E92ZMYYFZ', 'safe_sun_gazer', 'Easy to attach and provides crystal clear, safe views of the sun. The build quality is excellent. A must-have accessory for solar enthusiasts.', '5.0'),
        ('6E92ZMYYFZ', 'filter_fan', 'Works perfectly with my 8-inch scope. No more worries about accidental dislodgement. Great product for protecting your eyes and equipment.', '4.5'),
        ('6E92ZMYYFZ', 'eclipse_ready', 'Bought this for the upcoming eclipse, and it fits perfectly. Tested it out, and the views are fantastic and safe. Very happy with this purchase.', '5.0'),
  
        ('HQTGWGPNH4', 'history_buff', 'A fascinating glimpse into historical astronomical thought. The content is incredibly insightful. A must-read for anyone interested in the history of science.', '5.0'),
        ('HQTGWGPNH4', 'bookworm_astro', 'Beautifully presented historical document. It''s amazing to see how comets were understood centuries ago. A valuable addition to any astronomy library.', '4.5'),
        ('HQTGWGPNH4', 'ancient_texts', 'Such a unique and intriguing read. The historical context is captivating. It offers a different perspective on celestial events.', '4.0'),
        ('HQTGWGPNH4', 'celestial_history', 'I love historical astronomy, and this book delivers. It''s well-researched and provides a window into past beliefs. Highly recommended for scholars.', '5.0'),
        ('HQTGWGPNH4', 'rare_find', 'A truly special book for enthusiasts of astronomical history. The details about ancient astrologers are very interesting. Great for a deeper understanding.', '4.5');
  
    -- Product Catalog Service: create a schema
    CREATE SCHEMA catalog;
    GRANT USAGE ON SCHEMA catalog TO otelu;
  
    -- Product Catalog Service: create tables
    CREATE TABLE catalog.products (
        id TEXT PRIMARY KEY,
        name TEXT NOT NULL,
        description TEXT,
        picture TEXT,
        price_currency_code TEXT NOT NULL,
        price_units BIGINT NOT NULL,
        price_nanos INT NOT NULL,
        categories TEXT
    );
  
    -- Product Catalog Service: grant permission to schema
    GRANT SELECT ON ALL TABLES IN SCHEMA catalog TO otelu;
  
    -- Product Catalog Service: add product data
    INSERT INTO catalog.products (id, name, description, picture, price_currency_code, price_units, price_nanos, categories)
    VALUES
        ('OLJCESPC7Z', 'National Park Foundation Explorascope', 'The National Park Foundation''s (NPF) Explorascope 60AZ is a manual alt-azimuth, refractor telescope perfect for celestial viewing on the go. The NPF Explorascope 60 can view the planets, moon, star clusters and brighter deep sky objects like the Orion Nebula and Andromeda Galaxy.', 'NationalParkFoundationExplorascope.jpg', 'USD', 101, 960000000, 'telescopes'),
        ('66VCHSJNUP', 'Starsense Explorer Refractor Telescope', 'The first telescope that uses your smartphone to analyze the night sky and calculate its position in real time. StarSense Explorer is ideal for beginners thanks to the app''s user-friendly interface and detailed tutorials. It''s like having your own personal tour guide of the night sky', 'StarsenseExplorer.jpg', 'USD', 349, 950000000, 'telescopes'),
        ('1YMWWN1N4O', 'Eclipsmart Travel Refractor Telescope', 'Dedicated white-light solar scope for the observer on the go. The 50mm refracting solar scope uses Solar Safe, ISO compliant, full-aperture glass filter material to ensure the safest view of solar events.  The kit comes complete with everything you need, including the dedicated travel solar scope, a Solar Safe finderscope, tripod, a high quality 20mm (18x) Kellner eyepiece and a nylon backpack to carry everything in.  This Travel Solar Scope makes it easy to share the Sun as well as partial and total solar eclipses with the whole family and offers much higher magnifications than you would otherwise get using handheld solar viewers or binoculars.', 'EclipsmartTravelRefractorTelescope.jpg', 'USD', 129, 950000000, 'telescopes,travel'),
        ('L9ECAV7KIM', 'Lens Cleaning Kit', 'Wipe away dust, dirt, fingerprints and other particles on your lenses to see clearly with the Lens Cleaning Kit. This cleaning kit works on all glass and optical surfaces, including telescopes, binoculars, spotting scopes, monoculars, microscopes, and even your camera lenses, computer screens, and mobile devices.  The kit comes complete with a retractable lens brush to remove dust particles and dirt and two options to clean smudges and fingerprints off of your optics, pre-moistened lens wipes and a bottled lens cleaning fluid with soft cloth.', 'LensCleaningKit.jpg', 'USD', 21, 950000000, 'accessories'),
        ('2ZYFJ3GM2N', 'Roof Binoculars', 'This versatile, all-around binocular is a great choice for the trail, the stadium, the arena, or just about anywhere you want a close-up view of the action without sacrificing brightness or detail. It''s an especially great companion for nature observation and bird watching, with ED glass that helps you spot the subtlest field markings and a close focus of just 6.5 feet.', 'RoofBinoculars.jpg', 'USD', 209, 950000000, 'binoculars'),
        ('0PUK6V6EV0', 'Solar System Color Imager', 'You have your new telescope and have observed Saturn and Jupiter. Now you''re ready to take the next step and start imaging them. But where do you begin? The NexImage 10 Solar System Imager is the perfect solution.', 'SolarSystemColorImager.jpg', 'USD', 175, 0, 'accessories,telescopes'),
        ('LS4PSXUNUM', 'Red Flashlight', 'This 3-in-1 device features a 3-mode red flashlight, a hand warmer, and a portable power bank for recharging your personal electronics on the go. Whether you use it to light the way at an astronomy star party, a night walk, or wildlife research, ThermoTorch 3 Astro Red''s rugged, IPX4-rated design will withstand your everyday activities.', 'RedFlashlight.jpg', 'USD', 57, 80000000, 'accessories,flashlights'),
        ('9SIQT8TOJO', 'Optical Tube Assembly', 'Capturing impressive deep-sky astroimages is easier than ever with Rowe-Ackermann Schmidt Astrograph (RASA) V2, the perfect companion to today''s top DSLR or astronomical CCD cameras. This fast, wide-field f/2.2 system allows for shorter exposure times compared to traditional f/10 astroimaging, without sacrificing resolution. Because shorter sub-exposure times are possible, your equatorial mount won''t need to accurately track over extended periods. The short focal length also lessens equatorial tracking demands. In many cases, autoguiding will not be required.', 'OpticalTubeAssembly.jpg', 'USD', 3599, 0, 'accessories,telescopes,assembly'),
        ('6E92ZMYYFZ', 'Solar Filter', 'Enhance your viewing experience with EclipSmart Solar Filter for 8" telescopes. With two Velcro straps and four self-adhesive Velcro pads for added safety, you can be assured that the solar filter cannot be accidentally knocked off and will provide Solar Safe, ISO compliant viewing.', 'SolarFilter.jpg', 'USD', 69, 950000000, 'accessories,telescopes'),
        ('HQTGWGPNH4', 'The Comet Book', 'A 16th-century treatise on comets, created anonymously in Flanders (now northern France) and now held at the Universitätsbibliothek Kassel. Commonly known as The Comet Book (or Kometenbuch in German), its full title translates as "Comets and their General and Particular Meanings, According to Ptolomeé, Albumasar, Haly, Aliquind and other Astrologers". The image is from https://publicdomainreview.org/collection/the-comet-book, made available by the Universitätsbibliothek Kassel under a CC-BY SA 4.0 license (https://creativecommons.org/licenses/by-sa/4.0/)', 'TheCometBook.jpg', 'USD', 0, 990000000, 'books');
---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-collector
  labels:
    helm.sh/chart: opentelemetry-collector-0.142.2
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: otel-demo
    app.kubernetes.io/version: "0.142.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: opentelemetry-collector
    app.kubernetes.io/component: standalone-collector
rules:
  - apiGroups: [""]
    resources: ["pods", "namespaces"]
    verbs: ["get", "watch", "list"]
  - apiGroups: ["apps"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events", "namespaces", "namespaces/status", "nodes", "nodes/spec", "pods", "pods/status", "replicationcontrollers", "replicationcontrollers/status", "resourcequotas", "services" ]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["apps"]
    resources: ["daemonsets", "deployments", "replicasets", "statefulsets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["daemonsets", "deployments", "replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["batch"]
    resources: ["jobs", "cronjobs"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["autoscaling"]
    resources: ["horizontalpodautoscalers"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["nodes/stats"]
    verbs: ["get", "watch", "list"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "watch"]
---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-collector
  labels:
    helm.sh/chart: opentelemetry-collector-0.142.2
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: otel-demo
    app.kubernetes.io/version: "0.142.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: opentelemetry-collector
    app.kubernetes.io/component: standalone-collector
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: otel-collector
subjects:
- kind: ServiceAccount
  name: otel-collector
  namespace: opentelemetry-demo
---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: opentelemetry-demo
  labels:
    helm.sh/chart: opentelemetry-collector-0.142.2
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: otel-demo
    app.kubernetes.io/version: "0.142.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: opentelemetry-collector
    app.kubernetes.io/component: standalone-collector
    component: standalone-collector
spec:
  type: ClusterIP
  ports:
    
    - name: healthcheck
      port: 13133
      targetPort: 13133
      protocol: TCP
    - name: metrics
      port: 8888
      targetPort: 8888
      protocol: TCP
    - name: otlp
      port: 4317
      targetPort: 4317
      protocol: TCP
      appProtocol: grpc
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP
  selector:
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: otel-demo
    component: standalone-collector
  internalTrafficPolicy: Cluster
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: ad
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: ad
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: ad
    app.kubernetes.io/name: ad
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: ad
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: cart
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: cart
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: cart
    app.kubernetes.io/name: cart
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: cart
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: checkout
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: checkout
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: checkout
    app.kubernetes.io/name: checkout
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: checkout
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: currency
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: currency
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: currency
    app.kubernetes.io/name: currency
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: currency
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: email
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: email
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: email
    app.kubernetes.io/name: email
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: email
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: flagd
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: flagd
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: flagd
    app.kubernetes.io/name: flagd
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8013
      name: rpc
      targetPort: 8013
    - port: 8016
      name: ofrep
      targetPort: 8016
    - port: 4000
      name: tcp-service-0
      targetPort: 4000
  selector:
    
    opentelemetry.io/name: flagd
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: frontend
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: frontend
    app.kubernetes.io/name: frontend
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: frontend
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend-proxy
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: frontend-proxy
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: frontend-proxy
    app.kubernetes.io/name: frontend-proxy
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: frontend-proxy
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: image-provider
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: image-provider
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: image-provider
    app.kubernetes.io/name: image-provider
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8081
      name: tcp-service
      targetPort: 8081
  selector:
    
    opentelemetry.io/name: image-provider
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: kafka
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: kafka
    app.kubernetes.io/name: kafka
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 9092
      name: plaintext
      targetPort: 9092
    - port: 9093
      name: controller
      targetPort: 9093
  selector:
    
    opentelemetry.io/name: kafka
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: llm
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: llm
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: llm
    app.kubernetes.io/name: llm
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8000
      name: tcp-service
      targetPort: 8000
  selector:
    
    opentelemetry.io/name: llm
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: load-generator
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: load-generator
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: load-generator
    app.kubernetes.io/name: load-generator
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8089
      name: tcp-service
      targetPort: 8089
  selector:
    
    opentelemetry.io/name: load-generator
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: payment
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: payment
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: payment
    app.kubernetes.io/name: payment
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: payment
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgresql
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: postgresql
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: postgresql
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 5432
      name: tcp-service
      targetPort: 5432
  selector:
    
    opentelemetry.io/name: postgresql
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: product-catalog
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: product-catalog
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: product-catalog
    app.kubernetes.io/name: product-catalog
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: product-catalog
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: product-reviews
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: product-reviews
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: product-reviews
    app.kubernetes.io/name: product-reviews
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 3551
      name: tcp-service
      targetPort: 3551
  selector:
    
    opentelemetry.io/name: product-reviews
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: quote
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: quote
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: quote
    app.kubernetes.io/name: quote
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: quote
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: recommendation
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: recommendation
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: recommendation
    app.kubernetes.io/name: recommendation
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: recommendation
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: shipping
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: shipping
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: shipping
    app.kubernetes.io/name: shipping
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: shipping
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: valkey-cart
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: valkey-cart
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: valkey-cart
    app.kubernetes.io/name: valkey-cart
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 6379
      name: valkey-cart
      targetPort: 6379
  selector:
    
    opentelemetry.io/name: valkey-cart
---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: opentelemetry-demo
  labels:
    helm.sh/chart: opentelemetry-collector-0.142.2
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: otel-demo
    app.kubernetes.io/version: "0.142.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: opentelemetry-collector
    app.kubernetes.io/component: standalone-collector
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: opentelemetry-collector
      app.kubernetes.io/instance: otel-demo
      component: standalone-collector
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: 442f5efe361c693cf9e14ea32495eb46afeb6a2b4156b5f461cc13318665c813
        opentelemetry_community_demo: "true"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/name: opentelemetry-collector
        app.kubernetes.io/instance: otel-demo
        component: standalone-collector
        
    spec:
      
      serviceAccountName: otel-collector
      automountServiceAccountToken: true
      securityContext:
        {}
      containers:
        - name: opentelemetry-collector
          args:
            - --config=/conf/relay.yaml
          securityContext:
            {}
          image: "otel/opentelemetry-collector-contrib:0.142.0"
          imagePullPolicy: IfNotPresent
          ports:
            
            - name: healthcheck
              containerPort: 13133
              protocol: TCP
            - name: metrics
              containerPort: 8888
              protocol: TCP
            - name: otlp
              containerPort: 4317
              protocol: TCP
            - name: otlp-http
              containerPort: 4318
              protocol: TCP
          env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: K8S_NODE_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: GOMEMLIMIT
              value: "160MiB"
            - name: INTERNAL_TELEMETRY_SERVICE_NAME
              value: astroshop-otel-collector
            - name: KUBE_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: NR_LICENSE_KEY
              valueFrom:
                secretKeyRef:
                  key: license-key
                  name: newrelic-license-key
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
          livenessProbe:
            httpGet:
              path: /
              port: 13133
          readinessProbe:
            httpGet:
              path: /
              port: 13133
          resources:
            limits:
              memory: 200Mi
          volumeMounts:
            - mountPath: /conf
              name: opentelemetry-collector-configmap
            - name: hostfs
              mountPath: /hostfs
              readOnly: true
              mountPropagation: HostToContainer
      volumes:
        - name: opentelemetry-collector-configmap
          configMap:
            name: otel-collector
            items:
              - key: relay
                path: relay.yaml
        - name: hostfs
          hostPath:
            path: /
      hostNetwork: false
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: accounting
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: accounting
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: accounting
    app.kubernetes.io/name: accounting
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: accounting
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: accounting
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: accounting
        app.kubernetes.io/name: accounting
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: accounting
          image: 'ghcr.io/open-telemetry/demo:2.2.0-accounting'
          imagePullPolicy: IfNotPresent
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: KAFKA_ADDR
              value: kafka:9092
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318
            - name: DB_CONNECTION_STRING
              value: Host=postgresql;Username=otelu;Password=otelp;Database=otel
            - name: OTEL_DOTNET_AUTO_TRACES_ENTITYFRAMEWORKCORE_INSTRUMENTATION_ENABLED
              value: "false"
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 300Mi
            requests:
              memory: 200Mi
          volumeMounts:
      initContainers:
        - command:
          - sh
          - -c
          - until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;
          image: busybox:latest
          name: wait-for-kafka
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ad
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: ad
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: ad
    app.kubernetes.io/name: ad
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: ad
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: ad
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: ad
        app.kubernetes.io/name: ad
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: ad
          image: 'ghcr.io/open-telemetry/demo:2.2.0-ad'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: AD_PORT
              value: "8080"
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318
            - name: OTEL_LOGS_EXPORTER
              value: otlp
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 300Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cart
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: cart
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: cart
    app.kubernetes.io/name: cart
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: cart
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: cart
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: cart
        app.kubernetes.io/name: cart
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: cart
          image: 'ghcr.io/open-telemetry/demo:2.2.0-cart'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: CART_PORT
              value: "8080"
            - name: ASPNETCORE_URLS
              value: http://*:$(CART_PORT)
            - name: VALKEY_ADDR
              value: valkey-cart:6379
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 160Mi
          volumeMounts:
      initContainers:
        - command:
          - sh
          - -c
          - until nc -z -v -w30 valkey-cart 6379; do echo waiting for valkey-cart; sleep 2;
            done;
          image: busybox:latest
          name: wait-for-valkey-cart
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: checkout
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: checkout
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: checkout
    app.kubernetes.io/name: checkout
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: checkout
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: checkout
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: checkout
        app.kubernetes.io/name: checkout
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: checkout
          image: 'ghcr.io/open-telemetry/demo:2.2.0-checkout'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: CHECKOUT_PORT
              value: "8080"
            - name: CART_ADDR
              value: cart:8080
            - name: CURRENCY_ADDR
              value: currency:8080
            - name: EMAIL_ADDR
              value: http://email:8080
            - name: PAYMENT_ADDR
              value: payment:8080
            - name: PRODUCT_CATALOG_ADDR
              value: product-catalog:8080
            - name: SHIPPING_ADDR
              value: http://shipping:8080
            - name: KAFKA_ADDR
              value: kafka:9092
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: GOMEMLIMIT
              value: 16MiB
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 20Mi
          volumeMounts:
      initContainers:
        - command:
          - sh
          - -c
          - until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;
          image: busybox:latest
          name: wait-for-kafka
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: currency
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: currency
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: currency
    app.kubernetes.io/name: currency
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: currency
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: currency
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: currency
        app.kubernetes.io/name: currency
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: currency
          image: 'ghcr.io/open-telemetry/demo:2.2.0-currency'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: CURRENCY_PORT
              value: "8080"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: VERSION
              value: '2.2.0'
            - name: IPV6_ENABLED
              value: "false"
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 20Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: email
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: email
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: email
    app.kubernetes.io/name: email
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: email
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: email
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: email
        app.kubernetes.io/name: email
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: email
          image: 'ghcr.io/open-telemetry/demo:2.2.0-email'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: EMAIL_PORT
              value: "8080"
            - name: APP_ENV
              value: production
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 100Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flagd
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: flagd
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: flagd
    app.kubernetes.io/name: flagd
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: flagd
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: flagd
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: flagd
        app.kubernetes.io/name: flagd
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: flagd
          image: 'ghcr.io/open-feature/flagd:v0.12.9'
          imagePullPolicy: IfNotPresent
          command:
            - /flagd-build
            - start
            - --port
            - "8013"
            - --ofrep-port
            - "8016"
            - --uri
            - file:./etc/flagd/demo.flagd.json
          ports:
            
            - containerPort: 8013
              name: rpc
            - containerPort: 8016
              name: ofrep
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: FLAGD_METRICS_EXPORTER
              value: otel
            - name: FLAGD_OTEL_COLLECTOR_URI
              value: $(OTEL_COLLECTOR_NAME):4317
            - name: GOMEMLIMIT
              value: 60MiB
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 75Mi
          volumeMounts:
            - name: config-rw
              mountPath: /etc/flagd
        - name: flagd-ui
          image: 'ghcr.io/open-telemetry/demo:2.2.0-flagd-ui'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 4000
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: FLAGD_METRICS_EXPORTER
              value: otel
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318
            - name: FLAGD_UI_PORT
              value: "4000"
            - name: SECRET_KEY_BASE
              value: yYrECL4qbNwleYInGJYvVnSkwJuSQJ4ijPTx5tirGUXrbznFIBFVJdPl5t6O9ASw
            - name: PHX_HOST
              value: localhost
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 250Mi
          volumeMounts:
            - mountPath: /app/data
              name: config-rw
      initContainers:
        - command:
          - sh
          - -c
          - cp /config-ro/demo.flagd.json /config-rw/demo.flagd.json && cat /config-rw/demo.flagd.json
          image: busybox
          name: init-config
          volumeMounts:
          - mountPath: /config-ro
            name: config-ro
          - mountPath: /config-rw
            name: config-rw
      volumes:
        - name: config-rw
          emptyDir: {}
        - configMap:
            name: flagd-config
          name: config-ro
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fraud-detection
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: fraud-detection
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: fraud-detection
    app.kubernetes.io/name: fraud-detection
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: fraud-detection
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: fraud-detection
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: fraud-detection
        app.kubernetes.io/name: fraud-detection
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: fraud-detection
          image: 'ghcr.io/open-telemetry/demo:2.2.0-fraud-detection'
          imagePullPolicy: IfNotPresent
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: KAFKA_ADDR
              value: kafka:9092
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318
            - name: OTEL_INSTRUMENTATION_KAFKA_EXPERIMENTAL_SPAN_ATTRIBUTES
              value: "true"
            - name: OTEL_INSTRUMENTATION_MESSAGING_EXPERIMENTAL_RECEIVE_TELEMETRY_ENABLED
              value: "true"
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 300Mi
          volumeMounts:
      initContainers:
        - command:
          - sh
          - -c
          - until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;
          image: busybox:latest
          name: wait-for-kafka
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: frontend
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: frontend
    app.kubernetes.io/name: frontend
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: frontend
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: frontend
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: frontend
        app.kubernetes.io/name: frontend
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: frontend
          image: 'ghcr.io/open-telemetry/demo:2.2.0-frontend'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: FRONTEND_PORT
              value: "8080"
            - name: PORT
              value: $(FRONTEND_PORT)
            - name: FRONTEND_ADDR
              value: :8080
            - name: AD_ADDR
              value: ad:8080
            - name: CART_ADDR
              value: cart:8080
            - name: CHECKOUT_ADDR
              value: checkout:8080
            - name: CURRENCY_ADDR
              value: currency:8080
            - name: PRODUCT_CATALOG_ADDR
              value: product-catalog:8080
            - name: PRODUCT_REVIEWS_ADDR
              value: product-reviews:3551
            - name: RECOMMENDATION_ADDR
              value: recommendation:8080
            - name: SHIPPING_ADDR
              value: http://shipping:8080
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: ENV_PLATFORM
              value: kubernetes
            - name: OTEL_COLLECTOR_HOST
              value: $(OTEL_COLLECTOR_NAME)
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: WEB_OTEL_SERVICE_NAME
              value: frontend-web
            - name: PUBLIC_OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
              value: http://localhost:8080/otlp-http/v1/traces
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 250Mi
          securityContext:
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-proxy
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: frontend-proxy
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: frontend-proxy
    app.kubernetes.io/name: frontend-proxy
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: frontend-proxy
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: frontend-proxy
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: frontend-proxy
        app.kubernetes.io/name: frontend-proxy
      annotations:
        io.opentelemetry.discovery.metrics/config: |
          targets:
            - endpoint: "http://`endpoint`"
        io.opentelemetry.discovery.metrics/enabled: "true"
        io.opentelemetry.discovery.metrics/scraper: httpcheck
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: frontend-proxy
          image: 'ghcr.io/open-telemetry/demo:2.2.0-frontend-proxy'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: ENVOY_PORT
              value: "8080"
            - name: ENVOY_ADDR
              value: 0.0.0.0
            - name: ENVOY_ADMIN_PORT
              value: "10000"
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: FLAGD_UI_HOST
              value: flagd
            - name: FLAGD_UI_PORT
              value: "4000"
            - name: FRONTEND_HOST
              value: frontend
            - name: FRONTEND_PORT
              value: "8080"
            - name: GRAFANA_HOST
              value: grafana
            - name: GRAFANA_PORT
              value: "80"
            - name: IMAGE_PROVIDER_HOST
              value: image-provider
            - name: IMAGE_PROVIDER_PORT
              value: "8081"
            - name: JAEGER_HOST
              value: jaeger
            - name: JAEGER_UI_PORT
              value: "16686"
            - name: LOCUST_WEB_HOST
              value: load-generator
            - name: LOCUST_WEB_PORT
              value: "8089"
            - name: OTEL_COLLECTOR_HOST
              value: $(OTEL_COLLECTOR_NAME)
            - name: OTEL_COLLECTOR_PORT_GRPC
              value: "4317"
            - name: OTEL_COLLECTOR_PORT_HTTP
              value: "4318"
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 65Mi
          securityContext:
            runAsGroup: 101
            runAsNonRoot: true
            runAsUser: 101
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: image-provider
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: image-provider
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: image-provider
    app.kubernetes.io/name: image-provider
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: image-provider
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: image-provider
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: image-provider
        app.kubernetes.io/name: image-provider
      annotations:
        io.opentelemetry.discovery.metrics/config: |
          endpoint: "http://`endpoint`/status"
          collection_interval: "10s"
          timeout: "20s"
        io.opentelemetry.discovery.metrics/enabled: "true"
        io.opentelemetry.discovery.metrics/scraper: nginx
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: image-provider
          image: 'ghcr.io/open-telemetry/demo:2.2.0-image-provider'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 8081
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: IMAGE_PROVIDER_PORT
              value: "8081"
            - name: OTEL_COLLECTOR_PORT_GRPC
              value: "4317"
            - name: OTEL_COLLECTOR_HOST
              value: $(OTEL_COLLECTOR_NAME)
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 50Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: kafka
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: kafka
    app.kubernetes.io/name: kafka
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: kafka
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: kafka
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: kafka
        app.kubernetes.io/name: kafka
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: kafka
          image: 'ghcr.io/open-telemetry/demo:2.2.0-kafka'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 9092
              name: plaintext
            - containerPort: 9093
              name: controller
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: KAFKA_ADVERTISED_LISTENERS
              value: PLAINTEXT://kafka:9092
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318
            - name: KAFKA_HEAP_OPTS
              value: -Xmx400M -Xms400M
            - name: KAFKA_LISTENERS
              value: PLAINTEXT://:9092,CONTROLLER://:9093
            - name: KAFKA_CONTROLLER_LISTENER_NAMES
              value: CONTROLLER
            - name: KAFKA_CONTROLLER_QUORUM_VOTERS
              value: 1@kafka:9093
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 600Mi
          securityContext:
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: llm
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: llm
    app.kubernetes.io/name: llm
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: llm
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: llm
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: llm
        app.kubernetes.io/name: llm
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: llm
          image: 'ghcr.io/open-telemetry/demo:2.2.0-llm'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 8000
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            null
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: load-generator
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: load-generator
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: load-generator
    app.kubernetes.io/name: load-generator
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: load-generator
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: load-generator
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: load-generator
        app.kubernetes.io/name: load-generator
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: load-generator
          image: 'ghcr.io/open-telemetry/demo:2.2.0-load-generator'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 8089
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: LOCUST_WEB_HOST
              value: 0.0.0.0
            - name: LOCUST_WEB_PORT
              value: "8089"
            - name: LOCUST_USERS
              value: "10"
            - name: LOCUST_SPAWN_RATE
              value: "1"
            - name: LOCUST_HOST
              value: http://frontend-proxy:8080
            - name: LOCUST_HEADLESS
              value: "false"
            - name: LOCUST_AUTOSTART
              value: "true"
            - name: LOCUST_BROWSER_TRAFFIC_ENABLED
              value: "true"
            - name: PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION
              value: python
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: FLAGD_OFREP_PORT
              value: "8016"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 1500Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: payment
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: payment
    app.kubernetes.io/name: payment
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: payment
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: payment
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: payment
        app.kubernetes.io/name: payment
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: payment
          image: 'ghcr.io/open-telemetry/demo:2.2.0-payment'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: PAYMENT_PORT
              value: "8080"
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: IPV6_ENABLED
              value: "false"
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 140Mi
          securityContext:
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgresql
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: postgresql
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: postgresql
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: postgresql
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: postgresql
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: postgresql
        app.kubernetes.io/name: postgresql
      annotations:
        io.opentelemetry.discovery.metrics/config: |
          username: root
          password: otel
          metrics:
            postgresql.blks_hit:
              enabled: true
            postgresql.blks_read:
              enabled: true
            postgresql.tup_fetched:
              enabled: true
            postgresql.tup_returned:
              enabled: true
            postgresql.tup_inserted:
              enabled: true
            postgresql.tup_updated:
              enabled: true
            postgresql.tup_deleted:
              enabled: true
            postgresql.deadlocks:
              enabled: true
          tls:
            insecure: true
        io.opentelemetry.discovery.metrics/enabled: "true"
        io.opentelemetry.discovery.metrics/scraper: postgresql
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: postgresql
          image: 'postgres:17.6'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 5432
              name: service
          env:
            - name: POSTGRES_USER
              value: root
            - name: POSTGRES_PASSWORD
              value: otel
            - name: POSTGRES_DB
              value: otel
          resources:
            limits:
              memory: 100Mi
          volumeMounts:
            - name: postgresql-init
              mountPath: /docker-entrypoint-initdb.d
      volumes:
        - name: postgresql-init
          configMap:
            name: postgresql-init
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: product-catalog
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: product-catalog
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: product-catalog
    app.kubernetes.io/name: product-catalog
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: product-catalog
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: product-catalog
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: product-catalog
        app.kubernetes.io/name: product-catalog
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: product-catalog
          image: 'ghcr.io/open-telemetry/demo:2.2.0-product-catalog'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: PRODUCT_CATALOG_PORT
              value: "8080"
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: GOMEMLIMIT
              value: 16MiB
            - name: DB_CONNECTION_STRING
              value: postgres://otelu:otelp@postgresql/otel?sslmode=disable
            - name: OTEL_SEMCONV_STABILITY_OPT_IN
              value: database
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 20Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: product-reviews
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: product-reviews
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: product-reviews
    app.kubernetes.io/name: product-reviews
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: product-reviews
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: product-reviews
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: product-reviews
        app.kubernetes.io/name: product-reviews
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: product-reviews
          image: 'ghcr.io/open-telemetry/demo:2.2.0-product-reviews'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 3551
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: OPENAI_API_KEY
              value: dummy
            - name: LLM_MODEL
              value: astronomy-llm
            - name: LLM_HOST
              value: llm
            - name: LLM_PORT
              value: "8000"
            - name: LLM_BASE_URL
              value: http://$(LLM_HOST):$(LLM_PORT)/v1
            - name: PRODUCT_REVIEWS_PORT
              value: "3551"
            - name: PRODUCT_CATALOG_ADDR
              value: product-catalog:8080
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: DB_CONNECTION_STRING
              value: host=postgresql user=otelu password=otelp dbname=otel
            - name: PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION
              value: python
            - name: OTEL_PYTHON_LOG_CORRELATION
              value: "true"
            - name: OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT
              value: "true"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 100Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: quote
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: quote
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: quote
    app.kubernetes.io/name: quote
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: quote
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: quote
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: quote
        app.kubernetes.io/name: quote
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: quote
          image: 'ghcr.io/open-telemetry/demo:2.2.0-quote'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: QUOTE_PORT
              value: "8080"
            - name: OTEL_PHP_AUTOLOAD_ENABLED
              value: "true"
            - name: OTEL_PHP_INTERNAL_METRICS_ENABLED
              value: "true"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318
            - name: IPV6_ENABLED
              value: "false"
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 40Mi
          securityContext:
            runAsGroup: 33
            runAsNonRoot: true
            runAsUser: 33
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: recommendation
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: recommendation
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: recommendation
    app.kubernetes.io/name: recommendation
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: recommendation
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: recommendation
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: recommendation
        app.kubernetes.io/name: recommendation
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: recommendation
          image: 'ghcr.io/open-telemetry/demo:2.2.0-recommendation'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: RECOMMENDATION_PORT
              value: "8080"
            - name: PRODUCT_CATALOG_ADDR
              value: product-catalog:8080
            - name: OTEL_PYTHON_LOG_CORRELATION
              value: "true"
            - name: PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION
              value: python
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 500Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: shipping
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: shipping
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: shipping
    app.kubernetes.io/name: shipping
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: shipping
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: shipping
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: shipping
        app.kubernetes.io/name: shipping
      annotations:
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: shipping
          image: 'ghcr.io/open-telemetry/demo:2.2.0-shipping'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OTEL_K8S_CONTAINER_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OTEL_CLUSTER_NAME
              value: opentelemetry-demo
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: delta
            - name: SHIPPING_PORT
              value: "8080"
            - name: QUOTE_ADDR
              value: http://quote:8080
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: IPV6_ENABLED
              value: "false"
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME), service.namespace=opentelemetry-demo,
                service.version=2.2.0, service.instance.id=$(OTEL_POD_NAME),
                k8s.container.name=$(OTEL_K8S_CONTAINER_NAME), k8s.pod.name=$(OTEL_POD_NAME),
                k8s.namespace.name=$(OTEL_NAMESPACE_NAME), k8s.cluster.name=$(OTEL_CLUSTER_NAME)
          resources:
            limits:
              memory: 20Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: valkey-cart
  labels:
    helm.sh/chart: opentelemetry-demo-0.40.2
    
    opentelemetry.io/name: valkey-cart
    
    app.kubernetes.io/version: "2.2.0"
    app.kubernetes.io/component: valkey-cart
    app.kubernetes.io/name: valkey-cart
    app.kubernetes.io/part-of: opentelemetry-demo
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: valkey-cart
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: valkey-cart
        
        app.kubernetes.io/version: "2.2.0"
        app.kubernetes.io/component: valkey-cart
        app.kubernetes.io/name: valkey-cart
      annotations:
        io.opentelemetry.discovery.metrics/config: |
          username: valkey
          collection_interval: 10s
        io.opentelemetry.discovery.metrics/enabled: "true"
        io.opentelemetry.discovery.metrics/scraper: redis
        resource.opentelemetry.io/service.namespace: otel-demo
    spec:
      serviceAccountName: otel-demo
      containers:
        - name: valkey-cart
          image: 'valkey/valkey:9.0.1-alpine3.23'
          imagePullPolicy: IfNotPresent
          ports:
            
            - containerPort: 6379
              name: valkey-cart
          env:
            []
          resources:
            limits:
              memory: 20Mi
          securityContext:
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 999
          volumeMounts:
      volumes:

---
# Source: nr-k8s-otel-collector/charts/kube-state-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-6.1.5
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: nr-k8s-otel-collector
    app.kubernetes.io/version: "2.16.0"
  name: nr-k8s-otel-collector-kube-state-metrics
  namespace: opentelemetry-demo
---
# Source: nr-k8s-otel-collector/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nr-k8s-otel-collector
  namespace: opentelemetry-demo
  labels:
    app.kubernetes.io/instance: nr-k8s-otel-collector
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nr-k8s-otel-collector
    app.kubernetes.io/version: 1.2.0
    helm.sh/chart: nr-k8s-otel-collector-0.9.10
  annotations:
---
# Source: nr-k8s-otel-collector/templates/daemonset-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nr-k8s-otel-collector-daemonset-config
  namespace: opentelemetry-demo
  labels:
    app.kubernetes.io/instance: nr-k8s-otel-collector
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nr-k8s-otel-collector
    app.kubernetes.io/version: 1.2.0
    helm.sh/chart: nr-k8s-otel-collector-0.9.10
data:
  daemonset-config.yaml: |
    receivers:
      
      hostmetrics:
        # TODO (chris): this is a linux specific configuration
        root_path: /hostfs
        collection_interval: 1m
        scrapers:
          cpu:
            metrics:
              system.cpu.time:
                enabled: false
              system.cpu.utilization:
                enabled: true
              system.cpu.logical.count:
                enabled: true
          load:
          memory:
            metrics:
              system.memory.utilization:
                enabled: true
          paging:
            metrics:
              system.paging.utilization:
                enabled: false
              system.paging.faults:
                enabled: false
          filesystem:
            metrics:
              system.filesystem.utilization:
                enabled: true
          disk:
            metrics:
              system.disk.merged:
                enabled: false
              system.disk.pending_operations:
                enabled: false
              system.disk.weighted_io_time:
                enabled: false
          network:
            metrics:
              system.network.connections:
                enabled: false
          # Uncomment to enable process metrics, which can be noisy but valuable.
          # processes:
          # process:
          #   metrics:
          #     process.cpu.utilization:
          #       enabled: true
          #     process.cpu.time:
          #       enabled: false
          #   mute_process_name_error: true
          #   mute_process_exe_error: true
          #   mute_process_io_error: true
          #   mute_process_user_error: true

      kubeletstats:
        collection_interval: 1m
        endpoint: "${KUBE_NODE_NAME}:10250"
        auth_type: "serviceAccount"
        insecure_skip_verify: true
        metrics:
          k8s.container.cpu_limit_utilization:
            enabled: true
          k8s.pod.cpu_limit_utilization:
            enabled: true
          k8s.pod.cpu_request_utilization:
            enabled: true
          k8s.pod.memory_limit_utilization:
            enabled: true
          k8s.pod.memory_request_utilization:
            enabled: true

      prometheus:
        config:
          scrape_configs:
            - job_name: cadvisor
              scrape_interval: 1m
              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              kubernetes_sd_configs:
                - role: node
              relabel_configs:
                - replacement: kubernetes.default.svc.cluster.local:443
                  target_label: __address__
                - regex: (.+)
                  replacement: /api/v1/nodes/$${1}/proxy/metrics/cadvisor
                  source_labels:
                    - __meta_kubernetes_node_name
                  target_label: __metrics_path__
                - action: replace
                  target_label: job_label
                  replacement: cadvisor
                - source_labels: [__meta_kubernetes_node_name]
                  regex: ${KUBE_NODE_NAME}
                  action: keep
              scheme: https
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                insecure_skip_verify: false
                server_name: kubernetes
            - job_name: kubelet
              scrape_interval: 1m
              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              kubernetes_sd_configs:
                - role: node
              relabel_configs:
                - replacement: kubernetes.default.svc.cluster.local:443
                  target_label: __address__
                - regex: (.+)
                  replacement: /api/v1/nodes/$${1}/proxy/metrics
                  source_labels:
                    - __meta_kubernetes_node_name
                  target_label: __metrics_path__
                - action: replace
                  target_label: job_label
                  replacement: kubelet
                - source_labels: [__meta_kubernetes_node_name]
                  regex: ${KUBE_NODE_NAME}
                  action: keep
              scheme: https
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                insecure_skip_verify: false
                server_name: kubernetes

      filelog:
        include:
          - /var/log/pods/*/*/*.log
        exclude:
          # Exclude logs from opentelemetry containers
          # filelog paths for containerd and CRI-O
          - /var/log/pods/*/otel-collector-daemonset/*.log
          - /var/log/pods/*/otel-collector-deployment/*.log
          - /var/log/pods/*/containers/*-exec.log
          # konnectivity-agent is GKE specific (gke uses containerd as default)
          - /var/log/pods/*/konnectivity-agent/*.log
          # filelog paths for docker CRI
          - /var/log/container/otel-collector-daemonset/*.log
          - /var/log/container/otel-collector-deployment/*.log
          - /var/log/containers/*-exec.log
        include_file_path: true
        include_file_name: true
        operators:
        - id: container-parser
          type: container


    processors:
      
      

      groupbyattrs:
        keys:
          - pod
          - uid
          - container
          - daemonset
          - replicaset
          - statefulset
          - deployment
          - cronjob
          - configmap
          - job
          - job_name
          - horizontalpodautoscaler
          - persistentvolume
          - persistentvolumeclaim
          - endpoint
          - mutatingwebhookconfiguration
          - validatingwebhookconfiguration
          - lease
          - storageclass
          - secret
          - service
          - resourcequota
          - node
          - namespace

      transform/ksm:
        metric_statements:
          - delete_key(resource.attributes, "k8s.node.name")
          - delete_key(resource.attributes, "k8s.namespace.name")
          - delete_key(resource.attributes, "k8s.pod.uid")
          - delete_key(resource.attributes, "k8s.pod.name")
          - delete_key(resource.attributes, "k8s.container.name")
          - delete_key(resource.attributes, "k8s.replicaset.name")
          - delete_key(resource.attributes, "k8s.deployment.name")
          - delete_key(resource.attributes, "k8s.statefulset.name")
          - delete_key(resource.attributes, "k8s.daemonset.name")
          - delete_key(resource.attributes, "k8s.job.name")
          - delete_key(resource.attributes, "k8s.cronjob.name")
          - delete_key(resource.attributes, "k8s.replicationcontroller.name")
          - delete_key(resource.attributes, "k8s.hpa.name")
          - delete_key(resource.attributes, "k8s.resourcequota.name")
          - delete_key(resource.attributes, "k8s.volume.name")
          - set(resource.attributes["k8s.pod.uid"], resource.attributes["uid"])
          - set(resource.attributes["k8s.node.name"], resource.attributes["node"])
          - set(resource.attributes["k8s.namespace.name"], resource.attributes["namespace"])
          - set(resource.attributes["k8s.pod.name"], resource.attributes["pod"])
          - set(resource.attributes["k8s.container.name"], resource.attributes["container"])
          - set(resource.attributes["k8s.replicaset.name"], resource.attributes["replicaset"])
          - set(resource.attributes["k8s.deployment.name"], resource.attributes["deployment"])
          - set(resource.attributes["k8s.statefulset.name"], resource.attributes["statefulset"])
          - set(resource.attributes["k8s.daemonset.name"], resource.attributes["daemonset"])
          - set(resource.attributes["k8s.job.name"], resource.attributes["job_name"])
          - set(resource.attributes["k8s.cronjob.name"], resource.attributes["cronjob"])
          - set(resource.attributes["k8s.replicationcontroller.name"], resource.attributes["replicationcontroller"])
          - set(resource.attributes["k8s.hpa.name"], resource.attributes["horizontalpodautoscaler"])
          - set(resource.attributes["k8s.resourcequota.name"], resource.attributes["resourcequota"])
          - set(resource.attributes["k8s.volume.name"], resource.attributes["volumename"])
          - set(resource.attributes["k8s.volume.name"], resource.attributes["persistentvolume"])
          - set(resource.attributes["k8s.pvc.name"], resource.attributes["persistentvolumeclaim"])
          - delete_key(resource.attributes, "uid")
          - delete_key(resource.attributes, "node")
          - delete_key(resource.attributes, "namespace")
          - delete_key(resource.attributes, "pod")
          - delete_key(resource.attributes, "container")
          - delete_key(resource.attributes, "replicaset")
          - delete_key(resource.attributes, "deployment")
          - delete_key(resource.attributes, "statefulset")
          - delete_key(resource.attributes, "daemonset")
          - delete_key(resource.attributes, "job_name")
          - delete_key(resource.attributes, "cronjob")
          - delete_key(resource.attributes, "replicationcontroller")
          - delete_key(resource.attributes, "horizontalpodautoscaler")
          - delete_key(resource.attributes, "resourcequota")
          - delete_key(resource.attributes, "volumename")
          - delete_key(resource.attributes, "persistentvolume")
          - delete_key(resource.attributes, "persistentvolumeclaim")

      transform/extract_runtime:
        metric_statements:
          - context: datapoint
            conditions:
              - IsMatch(attributes["container_id"], ".*://.*")
            statements:
              - set(attributes["runtime"], Split(attributes["container_id"], "://")[0])
              - set(attributes["container_id"], Split(attributes["container_id"], "://")[1])

      metricstransform/ldm:
        transforms:
          - include: .*
            match_type: regexp
            action: update
            operations:
            - action: add_label
              new_label: low.data.mode
              new_value: 'false'

      metricstransform/kubeletstats:
        transforms:
          - include: container\.(cpu\.usage|filesystem\.(available|capacity|usage)|memory\.usage)
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: k8s\.node\.(cpu\.(time|usage)|filesystem\.(capacity|usage)|memory\.(available|working_set))
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: k8s\.pod\.(filesystem\.(available|capacity|usage)|memory\.working_set|network\.io)
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'

          - include: k8s\.pod\.(cpu|memory)_(limit|request)_utilization
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: k8s\.pod\.(cpu|memory)_request_limit_ratio
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'


      transform/collector:
        metric_statements:
          - set(datapoint.attributes["low.data.mode"], "true") where datapoint.attributes["job_label"] == "otel-collector-daemonset"

      metricstransform/k8s_cluster_info:
        transforms:
          - include: kubernetes_build_info
            action: update
            new_name: k8s.cluster.info

      metricstransform/cadvisor:
        transforms:
          - include: container_cpu_(cfs_(periods_total|throttled_periods_total)|usage_seconds_total)
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: container_memory_working_set_bytes
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: container_memory_mapped_file
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: container_network_(working_set_bytes|receive_(bytes_total|errors_total)|transmit_(bytes_total|errors_total))
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: container_spec_memory_limit_bytes
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'

      metricstransform/kubelet:
        transforms:
          - include: go_(goroutines|threads)
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: process_resident_memory_bytes
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: k8s.cluster.info
            action: update
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
                - value: 'false'
                  new_value: 'true'

      metricstransform/hostmetrics:
        transforms:
          - include: process\.(cpu\.utilization|disk\.io|memory\.(usage|virtual))
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: system\.cpu\.(utilization|load_average\.(15m|1m|5m))
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: system\.disk\.(io_time|operation_time|operations)
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: system\.(filesystem|memory)\.(usage|utilization)
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: system\.network\.(errors|io|packets)
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'

      # The metricsgeneration processor does not support operating on a metric that it previously made in the same processor
      metricsgeneration/calculate_percentage:
        rules:
            # Ratio of Requested Resources to Limits
            # request_limit_ratio = Limit_Utilization / Request_Utilization
            # = (usage / limit) / (usage / request) = (usage / limit) * (request / usage) = request / limit
          - name: k8s.pod.memory_request_limit_ratio
            type: calculate
            metric1: k8s.pod.memory_limit_utilization
            metric2: k8s.pod.memory_request_utilization
            operation: divide
          - name: k8s.pod.cpu_request_limit_ratio
            type: calculate
            metric1: k8s.pod.cpu_limit_utilization
            metric2: k8s.pod.cpu_request_utilization
            operation: divide
          - name: node.cpu.usage.percentage
            type: scale
            metric1: k8s.node.cpu.usage
            scale_by: <NODE_CPU_ALLOCATABLE_PLACEHOLDER>
            operation: divide
          - name: node.memory.usage.percentage
            type: scale
            metric1: k8s.node.memory.working_set
            scale_by: <NODE_MEMORY_ALLOCATABLE_PLACEHOLDER>
            operation: divide


      transform/tag_generated_metrics_ldm:
        metric_statements:
          - context: datapoint
            conditions:
              - metric.name == "k8s.pod.cpu_request_limit_ratio"
              - metric.name == "k8s.pod.memory_request_limit_ratio"
              - metric.name == "node.cpu.usage.percentage"
              - metric.name == "node.memory.usage.percentage"
            statements:
              - set(attributes["low.data.mode"], "true")

      filter/exclude_metrics_low_data_mode:
        metrics:
          metric:
            - 'HasAttrOnDatapoint("low.data.mode", "false")'

      transform/truncate:
        log_statements:
          - context: log
            statements:
              - truncate_all(log.attributes, 4095)
              - truncate_all(resource.attributes, 4095)

      # group system.cpu metrics by cpu
      metricstransform/hostmetrics_cpu:
        transforms:
          - include: system.cpu.utilization
            action: update
            operations:
              - action: aggregate_labels
                label_set: [ state ]
                aggregation_type: mean
          - include: system.paging.operations
            action: update
            operations:
              - action: aggregate_labels
                label_set: [ direction ]
                aggregation_type: sum

      # following system.% metrics reduce metrics reported by hostmetrics receiver
      filter/exclude_cpu_utilization:
        metrics:
          datapoint:
            - 'metric.name == "system.cpu.utilization" and attributes["state"] == "interrupt"'
            - 'metric.name == "system.cpu.utilization" and attributes["state"] == "nice"'
            - 'metric.name == "system.cpu.utilization" and attributes["state"] == "softirq"'
      filter/exclude_memory_utilization:
        metrics:
          datapoint:
            - 'metric.name == "system.memory.utilization" and attributes["state"] == "slab_unreclaimable"'
            - 'metric.name == "system.memory.utilization" and attributes["state"] == "inactive"'
            - 'metric.name == "system.memory.utilization" and attributes["state"] == "cached"'
            - 'metric.name == "system.memory.utilization" and attributes["state"] == "buffered"'
            - 'metric.name == "system.memory.utilization" and attributes["state"] == "slab_reclaimable"'
      filter/exclude_memory_usage:
        metrics:
          datapoint:
            - 'metric.name == "system.memory.usage" and attributes["state"] == "slab_unreclaimable"'
            - 'metric.name == "system.memory.usage" and attributes["state"] == "inactive"'
      filter/exclude_filesystem_utilization:
        metrics:
          datapoint:
            - 'metric.name == "system.filesystem.utilization" and attributes["type"] == "squashfs"'
      filter/exclude_filesystem_usage:
        metrics:
          datapoint:
            - 'metric.name == "system.filesystem.usage" and attributes["type"] == "squashfs"'
            - 'metric.name == "system.filesystem.usage" and attributes["state"] == "reserved"'
      filter/exclude_filesystem_inodes_usage:
        metrics:
          datapoint:
            - 'metric.name == "system.filesystem.inodes.usage" and attributes["type"] == "squashfs"'
            - 'metric.name == "system.filesystem.inodes.usage" and attributes["state"] == "reserved"'
      filter/exclude_system_disk:
        metrics:
          datapoint:
            - 'metric.name == "system.disk.operations" and IsMatch(attributes["device"], "^loop.*") == true'
            - 'metric.name == "system.disk.merged" and IsMatch(attributes["device"], "^loop.*") == true'
            - 'metric.name == "system.disk.io" and IsMatch(attributes["device"], "^loop.*") == true'
            - 'metric.name == "system.disk.io_time" and IsMatch(attributes["device"], "^loop.*") == true'
            - 'metric.name == "system.disk.operation_time" and IsMatch(attributes["device"], "^loop.*") == true'
      filter/exclude_system_paging:
        metrics:
          datapoint:
            - 'metric.name == "system.paging.usage" and attributes["state"] == "cached"'
            - 'metric.name == "system.paging.operations" and attributes["type"] == "cached"'
      filter/exclude_network:
        metrics:
          datapoint:
            - 'IsMatch(metric.name, "^system.network.*") == true and attributes["device"] == "lo"'

      filter/nr_exclude_container_zero_values:
        metrics:
          datapoint:
            - metric.name == "container_network_receive_errors_total" and value_double < 0.5
            - metric.name == "container_network_transmit_errors_total" and value_double < 0.5
            - metric.name == "container_network_transmit_bytes_total" and value_double < 0.5
            - metric.name == "container_network_receive_bytes_total" and value_double < 0.5


      attributes/exclude_system_paging:
        include:
          match_type: strict
          metric_names:
            - system.paging.operations
        actions:
          - key: type
            action: delete

      resourcedetection/env:
        detectors: ["env", "system"]
        override: false
        system:
          hostname_sources: ["os"]
          resource_attributes:
            host.name:
              enabled: false

      resourcedetection/cloudproviders:
        detectors: [gcp, eks, ec2, aks, azure]
        timeout: 2s
        override: false

      resource/newrelic:
        attributes:
          # We set the cluster name to what the customer specified in the helm chart
          - key: k8s.cluster.name
            action: upsert
            value: opentelemetry-demo
          - key: "newrelic.chart.version"
            action: upsert
            value: 0.9.10
          - key: newrelic.entity.type
            action: upsert
            value: "k8s"

      transform/low_data_mode_inator:
        metric_statements:
          - context: metric
            statements:
              - set(metric.description, "")
              - set(metric.unit, "")
          - context: datapoint
            statements:
              - delete_key(datapoint.attributes, "id")
              - delete_key(datapoint.attributes, "name")
              - delete_key(datapoint.attributes, "interface")
              - delete_key(datapoint.attributes, "cpu")

      resource/low_data_mode_inator:
        attributes:
          - key: http.scheme
            action: delete
          - key: net.host.name
            action: delete
          - key: net.host.port
            action: delete
          - key: url.scheme
            action: delete
          - key: server.address
            action: delete

      memory_limiter:
         check_interval: 1s
         limit_percentage: 80
         spike_limit_percentage: 25

      cumulativetodelta:

      k8sattributes/ksm:
        # Metadata attached by this processor is reliant on the uid & pod name. This would be sufficient for most types
        # of metrics but there are cases of metrics where a uid would not be present and thus metadata would
        # not be attached. To address cases like these, metadata attributes must be annotated in a different manner
        # such as by preserving some of the attributes presented by KSM.
        auth_type: "serviceAccount"
        passthrough: false
        filter:
          node_from_env_var: KUBE_NODE_NAME
        extract:
          metadata:
            - k8s.deployment.name
            - k8s.daemonset.name
            - k8s.namespace.name
            - k8s.node.name
            - k8s.pod.start_time
            - k8s.replicaset.name
            - k8s.statefulset.name
            - k8s.cronjob.name
            - k8s.job.name
        pod_association:
          - sources:
            - from: resource_attribute
              name: k8s.pod.uid
          - sources:
            - from: resource_attribute
              name: k8s.pod.name
      batch:
        send_batch_max_size: 1000
        timeout: 30s
        send_batch_size : 800

    exporters:
      
      

      otlphttp/newrelic:
        endpoint: "https://otlp.nr-data.net"
        headers:
          api-key: ${env:NR_LICENSE_KEY}

    connectors:
      

      routing/nr_logs_pipelines:
        default_pipelines: [logs/pipeline]
        table:
          - context: datapoint
            condition: "true"
            pipelines: [logs/pipeline]

      routing/logs_egress:
        default_pipelines: [logs/egress]
        table:
          - context: datapoint
            condition: "true"
            pipelines: [logs/egress]

      routing/metrics_egress:
        default_pipelines: [metrics/egress]
        table:
          - context: metric
            condition: "true"
            pipelines: [metrics/egress]

      routing/nr_metrics_pipelines:
        default_pipelines: [metrics/default]
        error_mode: propagate
        table:
          - context: metric
            condition: instrumentation_scope.name == "github.com/open-telemetry/opentelemetry-collector-contrib/receiver/hostmetricsreceiver/internal/scraper/networkscraper"
            pipelines: [metrics/nr]
          - context: metric
            condition: instrumentation_scope.name == "github.com/open-telemetry/opentelemetry-collector-contrib/receiver/hostmetricsreceiver/internal/scraper/loadscraper"
            pipelines: [metrics/nr]
          - context: metric
            condition: instrumentation_scope.name == "github.com/open-telemetry/opentelemetry-collector-contrib/receiver/hostmetricsreceiver/internal/scraper/diskscraper"
            pipelines: [metrics/nr]
          - context: metric
            condition: instrumentation_scope.name == "github.com/open-telemetry/opentelemetry-collector-contrib/receiver/hostmetricsreceiver/internal/scraper/memoryscraper"
            pipelines: [metrics/nr]
          - context: metric
            condition: instrumentation_scope.name == "github.com/open-telemetry/opentelemetry-collector-contrib/receiver/hostmetricsreceiver/internal/scraper/cpuscraper"
            pipelines: [metrics/nr]
          - context: metric
            condition: instrumentation_scope.name == "github.com/open-telemetry/opentelemetry-collector-contrib/receiver/hostmetricsreceiver/internal/scraper/filesystemscraper"
            pipelines: [metrics/nr]
          - context: metric
            condition: instrumentation_scope.name == "github.com/open-telemetry/opentelemetry-collector-contrib/receiver/hostmetricsreceiver/internal/scraper/pagingscraper"
            pipelines: [metrics/nr]
          - context: metric
            condition: instrumentation_scope.name == "github.com/open-telemetry/opentelemetry-collector-contrib/receiver/prometheusreceiver"
            pipelines: [metrics/nr_prometheus_cadv_kubelet]
          - context: metric
            condition: instrumentation_scope.name == "github.com/open-telemetry/opentelemetry-collector-contrib/receiver/kubeletstatsreceiver"
            pipelines: [metrics/nr]

    service:
      telemetry:
        metrics:
          readers:
      pipelines:
        
        metrics/ingress:
          receivers:
            - hostmetrics
            - kubeletstats
            - prometheus
          processors:
            - metricsgeneration/calculate_percentage
            
          exporters:
            - routing/nr_metrics_pipelines
            

        metrics/nr:
          receivers:
            - routing/nr_metrics_pipelines
          processors:
            - memory_limiter
            - metricstransform/k8s_cluster_info
            - metricstransform/ldm
            - transform/tag_generated_metrics_ldm
            - metricstransform/kubeletstats
            - metricstransform/cadvisor
            - metricstransform/kubelet
            - metricstransform/hostmetrics
            - filter/exclude_metrics_low_data_mode
            - metricstransform/hostmetrics_cpu
            - transform/truncate
            - filter/exclude_cpu_utilization
            - filter/exclude_memory_utilization
            - filter/exclude_memory_usage
            - filter/exclude_filesystem_utilization
            - filter/exclude_filesystem_usage
            - filter/exclude_filesystem_inodes_usage
            - filter/exclude_system_disk
            - filter/exclude_system_paging
            - filter/exclude_network
            - attributes/exclude_system_paging
            - resourcedetection/env
            - resourcedetection/cloudproviders
            - resource/newrelic
            - transform/low_data_mode_inator
            - resource/low_data_mode_inator
            - k8sattributes/ksm
            - cumulativetodelta
          exporters:
            - routing/metrics_egress
        metrics/nr_prometheus_cadv_kubelet:
          receivers:
            - routing/nr_metrics_pipelines
          processors:
            - memory_limiter
            - metricstransform/k8s_cluster_info
            - metricstransform/ldm
            - metricstransform/cadvisor
            - metricstransform/kubelet
            - transform/collector
            - filter/exclude_metrics_low_data_mode
            - filter/nr_exclude_container_zero_values
            - transform/truncate
            - resourcedetection/env
            - resourcedetection/cloudproviders
            - resource/newrelic
            - transform/low_data_mode_inator
            - resource/low_data_mode_inator
            - groupbyattrs
            - transform/ksm
            - k8sattributes/ksm
            - cumulativetodelta
          exporters:
            - routing/metrics_egress
        metrics/default:
          receivers:
            - routing/nr_metrics_pipelines
          processors:
            - memory_limiter
            - resource/newrelic
            - cumulativetodelta
          exporters:
            - routing/metrics_egress
        metrics/egress:
          receivers:
            - routing/metrics_egress
          processors:
            - transform/extract_runtime
            
            - batch
          exporters:
            - otlphttp/newrelic
            
        logs/ingress:
          receivers:
            - filelog
          processors:
            
          exporters:
            - routing/nr_logs_pipelines
            

        logs/pipeline:
          receivers:
            - routing/nr_logs_pipelines
          processors:
            - memory_limiter
            - transform/truncate
            - resource/newrelic
            - k8sattributes/ksm
          exporters:
            - routing/logs_egress

        logs/egress:
          receivers:
            - routing/logs_egress
          processors:
            
            - batch
          exporters:
            - otlphttp/newrelic
---
# Source: nr-k8s-otel-collector/templates/deployment-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nr-k8s-otel-collector-deployment-config
  namespace: opentelemetry-demo
  labels:
    app.kubernetes.io/instance: nr-k8s-otel-collector
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nr-k8s-otel-collector
    app.kubernetes.io/version: 1.2.0
    helm.sh/chart: nr-k8s-otel-collector-0.9.10
data:
  deployment-config.yaml: |
    receivers:
      
      otlp:
        protocols:
          http:
            endpoint: ${env:MY_POD_IP}:4318
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
      k8s_events:
      prometheus/ksm:
        config:
          scrape_configs:
            - job_name: kube-state-metrics
              scrape_interval: 1m
              kubernetes_sd_configs:
                - role: pod
              relabel_configs:
                - action: keep
                  regex: kube-state-metrics
                  source_labels:
                  - __meta_kubernetes_pod_label_app_kubernetes_io_name
                - action: replace
                  target_label: job_label
                  replacement: kube-state-metrics

      prometheus/controlplane:
        config:
          scrape_configs:
            - job_name: apiserver
              scrape_interval: 1m
              kubernetes_sd_configs:
                - role: endpoints
                  namespaces:
                    names:
                      - default
              scheme: https
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                insecure_skip_verify: false
              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              relabel_configs:
              - action: keep
                source_labels:
                - __meta_kubernetes_namespace
                - __meta_kubernetes_service_name
                - __meta_kubernetes_endpoint_port_name
                regex: default;kubernetes;https
              - action: replace
                source_labels:
                - __meta_kubernetes_namespace
                target_label: namespace
              - action: replace
                source_labels:
                - __meta_kubernetes_service_name
                target_label: service
              - action: replace
                target_label: job_label
                replacement: apiserver
            # if not running on openshift, this only works if controller-manager port 10257 is exposed in the pod
            # TODO: we may want to create our own service instead to expose the endpoint and scrape it instead
            - job_name: controller-manager
              scrape_interval: 1m
              metrics_path: /metrics
              kubernetes_sd_configs:
                - role: pod
                  namespaces:
                    names:
                      - kube-system
              scheme: https
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                insecure_skip_verify: false
              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              relabel_configs:
              - action: keep
                source_labels:
                - __meta_kubernetes_pod_name
                - __address__
                regex: .*controller-manager.*;.*:10257$
              - action: replace
                source_labels:
                - __meta_kubernetes_namespace
                target_label: namespace
              - action: replace
                source_labels:
                - __meta_kubernetes_pod_name
                target_label: pod
              - action: replace
                source_labels:
                - __meta_kubernetes_service_name
                target_label: service
              - action: replace
                target_label: job_label
                replacement: controller-manager
            # if not running on openshift, this only works if scheduler port 10259 is exposed in the pod
            # we may want to create our own service instead to expose the endpoint and scrape it instead
            - job_name: scheduler
              scrape_interval: 1m
              metrics_path: /metrics
              kubernetes_sd_configs:
                - role: pod
                  namespaces:
                    names:
                      - kube-system
              scheme: https
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                insecure_skip_verify: false
              bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              relabel_configs:
              - action: keep
                source_labels:
                - __meta_kubernetes_pod_name
                - __address__
                regex: .*scheduler.*;.*:10259$
              - action: replace
                source_labels:
                - __meta_kubernetes_namespace
                target_label: namespace
              - action: replace
                source_labels:
                - __meta_kubernetes_pod_name
                target_label: pod
              - action: replace
                source_labels:
                - __meta_kubernetes_service_name
                target_label: service
              - action: replace
                target_label: job_label
                replacement: scheduler

    processors:
      
      

      transform/promote_job_label:
        metric_statements:
        - context: datapoint
          statements:
          - set(instrumentation_scope.attributes["job_label"], datapoint.attributes["job_label"]) where datapoint.attributes["job_label"] != nil

      transform/remove_routing_metadata:
        metric_statements:
          - context: metric
            statements:
              - delete_key(instrumentation_scope.attributes, "job_label")

      transform/extract_runtime:
        metric_statements:
          - context: datapoint
            conditions:
              - IsMatch(attributes["container_id"], ".*://.*")
            statements:
              - set(attributes["runtime"], Split(attributes["container_id"], "://")[0])
              - set(attributes["container_id"], Split(attributes["container_id"], "://")[1])

      groupbyattrs:
        keys:
          - pod
          - uid
          - container
          - daemonset
          - replicaset
          - statefulset
          - deployment
          - cronjob
          - configmap
          - job
          - job_name
          - horizontalpodautoscaler
          - persistentvolume
          - persistentvolumeclaim
          - endpoint
          - mutatingwebhookconfiguration
          - validatingwebhookconfiguration
          - lease
          - storageclass
          - secret
          - service
          - resourcequota
          - node
          - namespace

      transform/ksm:
        metric_statements:
          - delete_key(resource.attributes, "k8s.node.name")
          - delete_key(resource.attributes, "k8s.namespace.name")
          - delete_key(resource.attributes, "k8s.pod.uid")
          - delete_key(resource.attributes, "k8s.pod.name")
          - delete_key(resource.attributes, "k8s.container.name")
          - delete_key(resource.attributes, "k8s.replicaset.name")
          - delete_key(resource.attributes, "k8s.deployment.name")
          - delete_key(resource.attributes, "k8s.statefulset.name")
          - delete_key(resource.attributes, "k8s.daemonset.name")
          - delete_key(resource.attributes, "k8s.job.name")
          - delete_key(resource.attributes, "k8s.cronjob.name")
          - delete_key(resource.attributes, "k8s.replicationcontroller.name")
          - delete_key(resource.attributes, "k8s.hpa.name")
          - delete_key(resource.attributes, "k8s.resourcequota.name")
          - delete_key(resource.attributes, "k8s.volume.name")
          - set(resource.attributes["k8s.pod.uid"], resource.attributes["uid"])
          - set(resource.attributes["k8s.node.name"], resource.attributes["node"])
          - set(resource.attributes["k8s.namespace.name"], resource.attributes["namespace"])
          - set(resource.attributes["k8s.pod.name"], resource.attributes["pod"])
          - set(resource.attributes["k8s.container.name"], resource.attributes["container"])
          - set(resource.attributes["k8s.replicaset.name"], resource.attributes["replicaset"])
          - set(resource.attributes["k8s.deployment.name"], resource.attributes["deployment"])
          - set(resource.attributes["k8s.statefulset.name"], resource.attributes["statefulset"])
          - set(resource.attributes["k8s.daemonset.name"], resource.attributes["daemonset"])
          - set(resource.attributes["k8s.job.name"], resource.attributes["job_name"])
          - set(resource.attributes["k8s.cronjob.name"], resource.attributes["cronjob"])
          - set(resource.attributes["k8s.replicationcontroller.name"], resource.attributes["replicationcontroller"])
          - set(resource.attributes["k8s.hpa.name"], resource.attributes["horizontalpodautoscaler"])
          - set(resource.attributes["k8s.resourcequota.name"], resource.attributes["resourcequota"])
          - set(resource.attributes["k8s.volume.name"], resource.attributes["persistentvolume"])
          - set(resource.attributes["k8s.pvc.name"], resource.attributes["persistentvolumeclaim"])
          - delete_key(resource.attributes, "uid")
          - delete_key(resource.attributes, "node")
          - delete_key(resource.attributes, "namespace")
          - delete_key(resource.attributes, "pod")
          - delete_key(resource.attributes, "container")
          - delete_key(resource.attributes, "replicaset")
          - delete_key(resource.attributes, "deployment")
          - delete_key(resource.attributes, "statefulset")
          - delete_key(resource.attributes, "daemonset")
          - delete_key(resource.attributes, "job_name")
          - delete_key(resource.attributes, "cronjob")
          - delete_key(resource.attributes, "replicationcontroller")
          - delete_key(resource.attributes, "horizontalpodautoscaler")
          - delete_key(resource.attributes, "resourcequota")
          - delete_key(resource.attributes, "persistentvolume")
          - delete_key(resource.attributes, "persistentvolumeclaim")

      transform/ksm_datapoints:
        metric_statements:
          - set(resource.attributes["k8s.volume.name"], datapoint.attributes["volumename"])
          - delete_key(datapoint.attributes, "volumename")

      metricstransform/k8s_cluster_info:
        transforms:
          - include: kubernetes_build_info
            action: update
            new_name: k8s.cluster.info

      metricstransform/kube_pod_container_status_phase:
        transforms:
          - include: 'kube_pod_container_status_waiting'
            match_type: strict
            action: update
            new_name: 'kube_pod_container_status_phase'
            operations:
            - action: add_label
              new_label: container_phase
              new_value: waiting
          - include: 'kube_pod_container_status_running'
            match_type: strict
            action: update
            new_name: 'kube_pod_container_status_phase'
            operations:
            - action: add_label
              new_label: container_phase
              new_value: running
          - include: 'kube_pod_container_status_terminated'
            match_type: strict
            action: update
            new_name: 'kube_pod_container_status_phase'
            operations:
            - action: add_label
              new_label: container_phase
              new_value: terminated

      metricstransform/ldm:
        transforms:
          - include: .*
            match_type: regexp
            action: update
            operations:
            - action: add_label
              new_label: low.data.mode
              new_value: 'false'

      metricstransform/k8s_cluster_info_ldm:
        transforms:
          - include: k8s.cluster.info
            action: update
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'

      transform/convert_timestamp:
        metric_statements:
          - context: datapoint
            conditions:
              - IsMatch(metric.name, "kube_pod_container_status_last_terminated_timestamp")
            statements:
              - set(datapoint.attributes["kube_pod_container_status_last_terminated_timestamp_formatted"], FormatTime(Unix(Int(datapoint.value_double)), "%Y-%m-%dT%H:%M:%SZ"))

      metricstransform/ksm:
        transforms:
          - include: kube_cronjob_(created|spec_suspend|status_(active|last_schedule_time))
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: kube_daemonset_(created|status_(current_number_scheduled|desired_number_scheduled|updated_number_scheduled)|status_number_(available|misscheduled|ready|unavailable))
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: kube_deployment_(created|metadata_generation|spec_(replicas|strategy_rollingupdate_max_surge)|status_(condition|observed_generation|replicas)|status_replicas_(available|ready|unavailable|updated)|labels|annotations)
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: kube_horizontalpodautoscaler_(spec_(max_replicas|min_replicas)|status_(condition|current_replicas|desired_replicas))
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: kube_job_(owner|complete|created|failed|spec_(active_deadline_seconds|completions|parallelism)|status_(active|completion_time|failed|start_time|succeeded))
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: kube_node_status_(allocatable|capacity|condition)
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: ^kube_namespace_(labels|annotations|status_phase|created)$$
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: kube_persistentvolume_(capacity_bytes|created|info|status_phase)
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: kube_persistentvolumeclaim_(created|info|resource_requests_storage_bytes|status_phase|access_mode)
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: kube_pod_container_(info|resource_(limits|requests)|status_(phase|ready|restarts_total|waiting_reason|last_terminated_timestamp|last_terminated_exitcode|last_terminated_reason))
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: ^kube_pod_(owner|created|info|status_(phase|ready|scheduled)|start_time|deletion_timestamp|labels|annotations)$$
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: ^kube_service_(annotations|created|info|labels|spec_type|status_load_balancer_ingress)$$
            action: update
            match_type: regexp
            operations:
              - action: update_label
                label: low.data.mode
                value_actions:
                  - value: 'false'
                    new_value: 'true'
          - include: kube_statefulset_(created|persistentvolumeclaim_retention_policy|replicas|status_(current_revision|replicas)|status_replicas_(available|current|ready|updated))
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: kube_replicaset_(owner|created)
            action: update
            match_type: regexp
            operations:
              - action: update_label
                label: low.data.mode
                value_actions:
                  - value: 'false'
                    new_value: 'true'

      metricstransform/apiserver:
        transforms:
          - include: apiserver_storage_objects
            action: update
            match_type: regexp
            operations:
              - action: update_label
                label: low.data.mode
                value_actions:
                  - value: 'false'
                    new_value: 'true'
          - include: go_(goroutines|threads)
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'
          - include: process_resident_memory_bytes
            action: update
            match_type: regexp
            operations:
            - action: update_label
              label: low.data.mode
              value_actions:
              - value: 'false'
                new_value: 'true'

      filter/exclude_metrics_low_data_mode:
        metrics:
          metric:
            - 'HasAttrOnDatapoint("low.data.mode", "false")'

      filter/exclude_zero_value_kube_node_status_condition:
        metrics:
          datapoint:
            - metric.name == "kube_node_status_condition" and value_double == 0.0

      filter/exclude_zero_value_kube_persistentvolumeclaim_status_phase:
        metrics:
          datapoint:
            - metric.name == "kube_persistentvolumeclaim_status_phase" and value_double == 0.0

      filter/nr_exclude_zero_value_kube_pod_container_deployment_statuses:
        metrics:
          datapoint:
            - metric.name == "kube_pod_status_phase" and value_double < 0.5
            - metric.name == "kube_pod_status_ready" and value_double < 0.5
            - metric.name == "kube_pod_status_scheduled" and value_double < 0.5
            - metric.name == "kube_pod_container_status_ready" and value_double < 0.5
            - metric.name == "kube_pod_container_status_phase" and value_double < 0.5
            - metric.name == "kube_pod_container_status_restarts_total" and value_double < 0.5
            - metric.name == "kube_deployment_status_condition" and value_double < 0.5
            - metric.name == "kube_pod_container_status_waiting_reason" and value_double < 0.5

      filter/nr_exclude_zero_value_kube_jobs:
        metrics:
          datapoint:
            - metric.name == "kube_job_complete" and value_double < 0.5
            - metric.name == "kube_job_spec_parallelism" and value_double < 0.5
            - metric.name == "kube_job_status_failed" and value_double < 0.5
            - metric.name == "kube_job_status_active" and value_double < 0.5
            - metric.name == "kube_job_status_succeeded" and value_double < 0.5

      resource/newrelic:
        attributes:
          # We set the cluster name to what the customer specified in the helm chart
          - key: k8s.cluster.name
            action: upsert
            value: opentelemetry-demo
          - key: "newrelic.chart.version"
            action: upsert
            value: 0.9.10
          - key: newrelic.entity.type
            action: upsert
            value: "k8s"

      resource/events:
        attributes:
          - key: "newrelic.event.type"
            action: upsert
            value: "OtlpInfrastructureEvent"
          - key: "category"
            action: upsert
            value: "kubernetes"
          - key: k8s.cluster.name
            action: upsert
            value: opentelemetry-demo
          - key: "newrelic.chart.version"
            action: upsert
            value: 0.9.10

      transform/events:
        log_statements:
          - context: log
            statements:
              - set(log.attributes["event.source.host"], resource.attributes["k8s.node.name"])

      transform/low_data_mode_inator:
        metric_statements:
          - context: metric
            statements:
              - set(metric.description, "")
              - set(metric.unit, "")

      resource/low_data_mode_inator:
        attributes:
          - key: http.scheme
            action: delete
          - key: net.host.name
            action: delete
          - key: net.host.port
            action: delete
          - key: url.scheme
            action: delete
          - key: server.address
            action: delete

      cumulativetodelta:
        exclude:
          metrics:
            - 'kube_pod_container_status_restarts_total'
          match_type: strict

      k8sattributes/ksm:
        # Metadata attached by this processor is reliant on the uid & pod name. This would be sufficient for most types
        # of metrics but there are cases of metrics such as kube_node* where a uid would not be present and thus metadata would
        # not be attached. To address cases like these, metadata attributes must be annotated in a different manner
        # such as by preserving some of the attributes presented by KSM.
        auth_type: "serviceAccount"
        passthrough: false
        extract:
          metadata:
            - k8s.deployment.name
            - k8s.daemonset.name
            - k8s.namespace.name
            - k8s.node.name
            - k8s.pod.start_time
            - k8s.replicaset.name
            - k8s.statefulset.name
            - k8s.cronjob.name
            - k8s.job.name
        pod_association:
          - sources:
            - from: resource_attribute
              name: k8s.pod.uid
          - sources:
            - from: resource_attribute
              name: k8s.pod.name

      attributes/self:
        actions:
          - key: k8s.node.name
            action: upsert
            from_attribute: node
          - key: k8s.namespace.name
            action: upsert
            from_attribute: namespace
          - key: k8s.pod.name
            action: upsert
            from_attribute: pod
          - key: k8s.container.name
            action: upsert
            from_attribute: container
          - key: k8s.replicaset.name
            action: upsert
            from_attribute: replicaset
          - key: k8s.deployment.name
            action: upsert
            from_attribute: deployment
          - key: k8s.statefulset.name
            action: upsert
            from_attribute: statefulset
          - key: k8s.daemonset.name
            action: upsert
            from_attribute: daemonset
          - key: k8s.job.name
            action: upsert
            from_attribute: job_name
          - key: k8s.cronjob.name
            action: upsert
            from_attribute: cronjob
          - key: k8s.replicationcontroller.name
            action: upsert
            from_attribute: replicationcontroller
          - key: k8s.hpa.name
            action: upsert
            from_attribute: horizontalpodautoscaler
          - key: k8s.resourcequota.name
            action: upsert
            from_attribute: resourcequota
          - key: k8s.volume.name
            action: upsert
            from_attribute: volumename
          - key: k8s.volume.name
            action: upsert
            from_attribute: persistentvolume
          - key: k8s.pvc.name
            action: upsert
            from_attribute: persistentvolumeclaim
          - key: node
            action: delete
          - key: namespace
            action: delete
          - key: pod
            action: delete
          - key: container
            action: delete
          - key: replicaset
            action: delete
          - key: deployment
            action: delete
          - key: statefulset
            action: delete
          - key: daemonset
            action: delete
          - key: job_name
            action: delete
          - key: cronjob
            action: delete
          - key: replicationcontroller
            action: delete
          - key: horizontalpodautoscaler
            action: delete
          - key: resourcequota
            action: delete
          - key: volumename
            action: delete
          - key: persistentvolume
            action: delete
          - key: persistentvolumeclaim
            action: delete

      memory_limiter:
         check_interval: 1s
         limit_percentage: 80
         spike_limit_percentage: 25

      batch:
        send_batch_max_size: 1000
        timeout: 30s
        send_batch_size : 800

    exporters:
      
      
      otlphttp/newrelic:
        endpoint: "https://otlp.nr-data.net"
        headers:
          api-key: ${env:NR_LICENSE_KEY}

    connectors:
      

      routing/nr_logs_pipelines:
        default_pipelines: [logs/pipeline]
        table:
          - context: datapoint
            condition: "true"
            pipelines: [logs/pipeline]

      routing/logs_egress:
        default_pipelines: [logs/egress]
        table:
          - context: datapoint
            condition: "true"
            pipelines: [logs/egress]

      routing/metrics_egress:
        default_pipelines: [metrics/egress]
        table:
          - context: metric
            condition: "true"
            pipelines: [metrics/egress]

      routing/nr_metrics_pipelines:
        default_pipelines: [metrics/default]
        error_mode: propagate
        table:
          - context: metric
            condition: instrumentation_scope.attributes["job_label"] == "kube-state-metrics"
            pipelines: [metrics/nr_ksm]
          - context: metric
            condition: instrumentation_scope.attributes["job_label"] == "apiserver"
            pipelines: [metrics/nr_controlplane]
          - context: metric
            condition: instrumentation_scope.attributes["job_label"] == "controller-manager"
            pipelines: [metrics/nr_controlplane]
          - context: metric
            condition: instrumentation_scope.attributes["job_label"] == "scheduler"
            pipelines: [metrics/nr_controlplane]

    service:
      telemetry:
        metrics:
          readers:
      pipelines:
        
        metrics/ingress:
          receivers:
            - prometheus/ksm
            - prometheus/controlplane
          processors:
            
            - transform/promote_job_label
          exporters:
            - routing/nr_metrics_pipelines
            

        metrics/nr_ksm:
          receivers:
            - routing/nr_metrics_pipelines
          processors:
            - memory_limiter
            - metricstransform/kube_pod_container_status_phase
            - filter/exclude_zero_value_kube_node_status_condition
            - filter/exclude_zero_value_kube_persistentvolumeclaim_status_phase
            - filter/nr_exclude_zero_value_kube_pod_container_deployment_statuses
            - transform/convert_timestamp
            - metricstransform/ldm
            - metricstransform/k8s_cluster_info_ldm
            - metricstransform/ksm
            - filter/exclude_metrics_low_data_mode
            - filter/nr_exclude_zero_value_kube_jobs
            - transform/low_data_mode_inator
            - resource/low_data_mode_inator
            - resource/newrelic
            - groupbyattrs
            - transform/ksm
            - transform/ksm_datapoints
            - k8sattributes/ksm
            - cumulativetodelta
          exporters:
            - routing/metrics_egress
        metrics/nr_controlplane:
          receivers:
            - routing/nr_metrics_pipelines
          processors:
            - memory_limiter
            - metricstransform/k8s_cluster_info
            - metricstransform/ldm
            - metricstransform/k8s_cluster_info_ldm
            - metricstransform/apiserver
            - filter/exclude_metrics_low_data_mode
            - transform/low_data_mode_inator
            - resource/low_data_mode_inator
            - resource/newrelic
            - attributes/self
            - cumulativetodelta
          exporters:
            - routing/metrics_egress
        metrics/default:
          receivers:
            - routing/nr_metrics_pipelines
          processors:
            - memory_limiter
            - resource/newrelic
            - cumulativetodelta
          exporters:
            - routing/metrics_egress
        metrics/egress:
          receivers:
            - routing/metrics_egress
          processors:
            - transform/remove_routing_metadata
            - transform/extract_runtime
            
            - batch
          exporters:
            - otlphttp/newrelic
            
        logs/ingress:
          receivers:
            - k8s_events
          processors:
            
          exporters:
            - routing/nr_logs_pipelines
            
        logs/pipeline:
          receivers:
            - routing/nr_logs_pipelines
          processors:
            - memory_limiter
            - transform/events
            - resource/events
            - resource/newrelic
          exporters:
            - routing/logs_egress
        logs/egress:
          receivers:
            - routing/logs_egress
          processors:
            
            - batch
          exporters:
            - otlphttp/newrelic
---
# Source: nr-k8s-otel-collector/charts/kube-state-metrics/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-6.1.5
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: nr-k8s-otel-collector
    app.kubernetes.io/version: "2.16.0"
  name: nr-k8s-otel-collector-kube-state-metrics
rules:

- apiGroups: ["certificates.k8s.io"]
  resources:
  - certificatesigningrequests
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - cronjobs
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - daemonsets
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - deployments
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - endpoints
  verbs: ["list", "watch"]

- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - ingresses
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - jobs
  verbs: ["list", "watch"]

- apiGroups: ["coordination.k8s.io"]
  resources:
  - leases
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - limitranges
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - mutatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - namespaces
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - networkpolicies
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - nodes
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumeclaims
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumes
  verbs: ["list", "watch"]

- apiGroups: ["policy"]
  resources:
    - poddisruptionbudgets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - pods
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - replicasets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - replicationcontrollers
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - resourcequotas
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - secrets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - services
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - storageclasses
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - validatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - volumeattachments
  verbs: ["list", "watch"]
---
# Source: nr-k8s-otel-collector/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: nr-k8s-otel-collector
  labels:
    app.kubernetes.io/instance: nr-k8s-otel-collector
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nr-k8s-otel-collector
    app.kubernetes.io/version: 1.2.0
    helm.sh/chart: nr-k8s-otel-collector-0.9.10
rules:
  - apiGroups:
    - ""
    resources:
    # following required for k8s_events, k8s_cluster receiver
    - events
    # following required for k8s_cluster receiver
    - namespaces
    # following required for prometheus, k8s_cluster receiver
    - nodes
    - nodes/metrics
    - pods
    - pods/status
    - services
    - endpoints
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    # following required for hostmetrics, prometheus receiver
    - nodes/spec
    - nodes/stats
    - nodes/proxy
    # following required for filelog receiver
    - pods/logs
    verbs:
    - get
  # following required for prometheus receiver
  - apiGroups:
    - ""
    resources:
    - replicationcontrollers
    - resourcequotas
    verbs:
    - list
    - watch
  # following required for prometheus receiver
  - apiGroups:
    - apps
    resources:
    - daemonsets
    - deployments
    - replicasets
    - statefulsets
    verbs:
    - get
    - list
    - watch
  # following required for prometheus receiver
  - apiGroups:
    - batch
    resources:
    - jobs
    - cronjobs
    verbs:
    - list
    - watch
  # following required for prometheus receiver
  - apiGroups:
    - autoscaling
    resources:
    - horizontalpodautoscalers
    verbs:
    - list
    - watch
  # following required for prometheus receiver
  - nonResourceURLs: ["/metrics"]
    verbs: ["get"]
---
# Source: nr-k8s-otel-collector/charts/kube-state-metrics/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-6.1.5
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: nr-k8s-otel-collector
    app.kubernetes.io/version: "2.16.0"
  name: nr-k8s-otel-collector-kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nr-k8s-otel-collector-kube-state-metrics
subjects:
- kind: ServiceAccount
  name: nr-k8s-otel-collector-kube-state-metrics
  namespace: opentelemetry-demo
---
# Source: nr-k8s-otel-collector/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: nr-k8s-otel-collector
  labels:
    app.kubernetes.io/instance: nr-k8s-otel-collector
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nr-k8s-otel-collector
    app.kubernetes.io/version: 1.2.0
    helm.sh/chart: nr-k8s-otel-collector-0.9.10
subjects:
  - kind: ServiceAccount
    name: nr-k8s-otel-collector
    namespace: opentelemetry-demo
roleRef:
  kind: ClusterRole
  name: nr-k8s-otel-collector
  apiGroup: rbac.authorization.k8s.io
---
# Source: nr-k8s-otel-collector/charts/kube-state-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: nr-k8s-otel-collector-kube-state-metrics
  namespace: opentelemetry-demo
  labels:    
    helm.sh/chart: kube-state-metrics-6.1.5
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: nr-k8s-otel-collector
    app.kubernetes.io/version: "2.16.0"
  annotations:
spec:
  type: "ClusterIP"
  ports:
  - name: "http"
    protocol: TCP
    port: 8080
    targetPort: 8080
  
  selector:    
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: nr-k8s-otel-collector
---
# Source: nr-k8s-otel-collector/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: nr-k8s-otel-collector-gateway
  namespace: opentelemetry-demo
  labels:
    app.kubernetes.io/instance: nr-k8s-otel-collector
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nr-k8s-otel-collector
    app.kubernetes.io/version: 1.2.0
    helm.sh/chart: nr-k8s-otel-collector-0.9.10
spec:
  type: ClusterIP
  ports:
    - name: otlp-http # Default endpoint for OpenTelemetry HTTP receiver.
      port: 4318
      protocol: TCP
      targetPort: 4318
    - name: otlp-grpc
      port: 4317
      protocol: TCP
      targetPort: 4317
  selector:
    app.kubernetes.io/instance: nr-k8s-otel-collector
    app.kubernetes.io/name: nr-k8s-otel-collector
    component: deployment
  internalTrafficPolicy: Cluster
---
# Source: nr-k8s-otel-collector/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nr-k8s-otel-collector-daemonset
  namespace: opentelemetry-demo
  labels:
    app.kubernetes.io/instance: nr-k8s-otel-collector
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nr-k8s-otel-collector
    app.kubernetes.io/version: 1.2.0
    helm.sh/chart: nr-k8s-otel-collector-0.9.10
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: nr-k8s-otel-collector
      app.kubernetes.io/name: nr-k8s-otel-collector
      component: daemonset
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: nr-k8s-otel-collector
        app.kubernetes.io/name: nr-k8s-otel-collector
        component: daemonset
      annotations:
        checksum/config: cce8229787eaa2594b38a40796df41804511fecb1136ce2553eb33b6225a829c
    spec:
      serviceAccountName: nr-k8s-otel-collector
      initContainers:
        - name: get-cpu-allocatable
          image: "docker.io/bitnami/kubectl:latest"
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              NODE_NAME=$(echo $KUBE_NODE_NAME)
              echo "Node Name: $KUBE_NODE_NAME"

              export NODE_CPU_ALLOCATABLE=$(kubectl get node $NODE_NAME -o jsonpath='{.status.allocatable.cpu}')

              if [[ -z "NODE_CPU_ALLOCATABLE" ]] || [[ "NODE_CPU_ALLOCATABLE" == "0" ]]; then
                echo "Could not retrieve CPU allocatable for node $NODE_NAME"
                exit 1
              fi

              # Convert milliCPU values to plain CPU values
              if [[ $NODE_CPU_ALLOCATABLE == *m ]]; then
                # Strip the 'm' and convert the milliCPUs to CPUs
                export NODE_CPU_ALLOCATABLE=$(awk "BEGIN {print ${NODE_CPU_ALLOCATABLE%?} / 1000}")
              fi

              export NODE_MEMORY_ALLOCATABLE=$(kubectl get node $NODE_NAME -o jsonpath='{.status.allocatable.memory}' | numfmt --from=auto)

              if [[ -z "NODE_MEMORY_ALLOCATABLE" ]] || [[ "NODE_MEMORY_ALLOCATABLE" == "0" ]]; then
                echo "Could not retrieve Memory allocatable for node $NODE_NAME"
                exit 1
              fi

              echo "NODE_CPU_ALLOCATABLE : $NODE_CPU_ALLOCATABLE"
              echo "NODE_MEMORY_ALLOCATABLE : $NODE_MEMORY_ALLOCATABLE"
              cp /temp-config/daemonset-config.yaml /final-config
              yq -i '(.processors.metricsgeneration/calculate_percentage.rules[] | select(.name == "node.cpu.usage.percentage").scale_by) = env(NODE_CPU_ALLOCATABLE)' /final-config/daemonset-config.yaml
              yq -i '(.processors.metricsgeneration/calculate_percentage.rules[] | select(.name == "node.memory.usage.percentage").scale_by) = env(NODE_MEMORY_ALLOCATABLE)' /final-config/daemonset-config.yaml
              cat /final-config/daemonset-config.yaml
          env:
            - name: KUBE_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
          resources:
            {}
          volumeMounts:
            - name: daemonset-config
              mountPath: /temp-config
            - name: final-daemonset-config
              mountPath: /final-config
      containers:
        - name: otel-collector-daemonset
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
          image: "docker.io/newrelic/nrdot-collector-k8s:1.5.0"
          imagePullPolicy: IfNotPresent
          args:
            - --config
            - /config/daemonset-config.yaml
            - --feature-gates
            - metricsgeneration.MatchAttributes
          resources:
            {}
          env:
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_UID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid
            - name: KUBE_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.instance.id=$(POD_NAME),k8s.pod.uid=$(POD_UID)
            - name: NR_LICENSE_KEY
              valueFrom:
                secretKeyRef:
                  name: newrelic-license-key
                  key: license-key
          volumeMounts:
            - name: host-fs
              mountPath: /hostfs
              readOnly: true
            - name: varlogpods
              mountPath: /var/log/pods
              readOnly: true
            - name: final-daemonset-config
              mountPath: /config
      volumes:
        - name: host-fs
          hostPath:
            path: /
        - name: varlogpods
          hostPath:
            path: /var/log/pods
        - name: final-daemonset-config
          emptyDir: {}
        - name: daemonset-config
          configMap:
            name: nr-k8s-otel-collector-daemonset-config
---
# Source: nr-k8s-otel-collector/charts/kube-state-metrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nr-k8s-otel-collector-kube-state-metrics
  namespace: opentelemetry-demo
  labels:    
    helm.sh/chart: kube-state-metrics-6.1.5
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: nr-k8s-otel-collector
    app.kubernetes.io/version: "2.16.0"
spec:
  selector:
    matchLabels:      
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: nr-k8s-otel-collector
  replicas: 1
  strategy:
    type: RollingUpdate
  revisionHistoryLimit: 10
  template:
    metadata:
      labels:        
        helm.sh/chart: kube-state-metrics-6.1.5
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: kube-state-metrics
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/instance: nr-k8s-otel-collector
        app.kubernetes.io/version: "2.16.0"
    spec:
      automountServiceAccountToken: true
      hostNetwork: false
      serviceAccountName: nr-k8s-otel-collector-kube-state-metrics
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      dnsPolicy: ClusterFirst
      containers:
      - name: kube-state-metrics
        args:
        - --port=8080
        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
        - --metric-labels-allowlist=pods=[*],namespaces=[*],deployments=[*]
        - --metric-annotations-allowlist=pods=[*],namespaces=[*],deployments=[*]
        imagePullPolicy: IfNotPresent
        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.16.0
        ports:
        - containerPort: 8080
          name: "http"
        livenessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /livez
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /readyz
            port: 8081
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          {}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
---
# Source: nr-k8s-otel-collector/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nr-k8s-otel-collector-deployment
  namespace: opentelemetry-demo
  labels:
    app.kubernetes.io/instance: nr-k8s-otel-collector
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nr-k8s-otel-collector
    app.kubernetes.io/version: 1.2.0
    helm.sh/chart: nr-k8s-otel-collector-0.9.10
spec:
  replicas: 1
  minReadySeconds: 5
  selector:
    matchLabels:
      app.kubernetes.io/instance: nr-k8s-otel-collector
      app.kubernetes.io/name: nr-k8s-otel-collector
      component: deployment
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: nr-k8s-otel-collector
        app.kubernetes.io/name: nr-k8s-otel-collector
        component: deployment
      annotations:
        checksum/config: 47239ba4324ee914628d0c4e7cd94da97b85113a41990fceb64babda8b98f813
    spec:
      serviceAccountName: nr-k8s-otel-collector
      containers:
        - name: otel-collector-deployment
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
          image: "docker.io/newrelic/nrdot-collector-k8s:1.5.0"
          imagePullPolicy: IfNotPresent
          args:
            - --config
            - /config/deployment-config.yaml
          resources:
            {}
          env:
            - name: KUBE_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: NR_LICENSE_KEY
              valueFrom:
                secretKeyRef:
                  name: newrelic-license-key
                  key: license-key
          ports:
            - name: http
              containerPort: 4318
              protocol: TCP
            - name: grpc
              containerPort: 4317
              protocol: TCP
          volumeMounts:
            - name: deployment-config
              mountPath: /config
      volumes:
        - name: deployment-config
          configMap:
            name: nr-k8s-otel-collector-deployment-config

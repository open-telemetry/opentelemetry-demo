# Observability Example: Cache Failure Scenario

## Overview

When you inject a **cache failure** using `./incidentfox/scripts/trigger-incident.sh cache-failure`, here's exactly what metrics, logs, and traces will show anomalies.

## Observability Endpoints

All accessible via Docker Compose (default) or Kubernetes:

```yaml
Prometheus:  http://localhost:9090
Grafana:     http://localhost:8080/grafana
Jaeger:      http://localhost:8080/jaeger/ui
OpenSearch:  http://localhost:9200
```

Full details: [`agent-config/endpoints.yaml`](../agent-config/endpoints.yaml)

---

## Timeline: Cache Failure Scenario

### T+0s: Trigger the Failure

```bash
./incidentfox/scripts/trigger-incident.sh cache-failure
```

This enables the `recommendationCacheFailure` feature flag, causing the recommendation service cache to fail.

---

## What You'll See in Observability Tools

### 1. Prometheus Metrics (T+0s to T+30s)

#### âœ… Primary Signals - Recommendation Service

**Query: Latency Spike**
```promql
histogram_quantile(0.95, 
  rate(http_server_duration_bucket{service_name="recommendation"}[5m])
)
```

**Before:**
```
Value: 0.050 (50ms)
```

**After (T+10s):**
```
Value: 0.520 (520ms)  â¬†ï¸ 10x INCREASE
```

---

**Query: Cache Hit Rate**
```promql
recommendation_cache_hit_rate
```

**Before:**
```
Value: 0.94 (94% hit rate)
```

**After (T+5s):**
```
Value: 0.00 (0% hit rate)  â¬‡ï¸ DROPPED TO ZERO
```

---

**Query: CPU Usage**
```promql
rate(process_cpu_seconds_total{service_name="recommendation"}[1m])
```

**Before:**
```
Value: 0.20 (20% CPU)
```

**After (T+15s):**
```
Value: 0.60 (60% CPU)  â¬†ï¸ 3x INCREASE
```

---

**Query: Request Rate to Product Catalog**
```promql
rate(http_client_requests_total{
  service_name="recommendation",
  target="product-catalog"
}[1m])
```

**Before:**
```
Value: 10 req/s
```

**After (T+10s):**
```
Value: 100 req/s  â¬†ï¸ 10x INCREASE (cache bypass)
```

---

#### âœ… Secondary Signals - Product Catalog Service (Downstream)

**Query: Request Rate Received**
```promql
rate(http_server_requests_total{service_name="product-catalog"}[1m])
```

**Before:**
```
Value: 95 req/s
```

**After (T+60s):**
```
Value: 980 req/s  â¬†ï¸ 10x INCREASE
```

---

**Query: CPU Usage**
```promql
rate(process_cpu_seconds_total{service_name="product-catalog"}[1m])
```

**Before:**
```
Value: 0.15 (15% CPU)
```

**After (T+60s):**
```
Value: 0.68 (68% CPU)  â¬†ï¸ 4.5x INCREASE
```

---

**Query: Latency**
```promql
histogram_quantile(0.95,
  rate(http_server_duration_bucket{service_name="product-catalog"}[5m])
)
```

**Before:**
```
Value: 0.022 (22ms)
```

**After (T+60s):**
```
Value: 0.125 (125ms)  â¬†ï¸ 5.7x INCREASE
```

---

#### âœ… Tertiary Signals - Frontend (End User Impact)

**Query: Page Load Time**
```promql
histogram_quantile(0.95,
  rate(http_server_duration_bucket{service_name="frontend"}[5m])
)
```

**Before:**
```
Value: 0.200 (200ms)
```

**After (T+60s):**
```
Value: 0.800 (800ms)  â¬†ï¸ 4x INCREASE
```

---

### 2. Grafana Dashboards

#### Dashboard: "Demo Dashboard"
Location: `http://localhost:8080/grafana/d/demo`

**Panels showing anomalies:**

1. **Service Latency (P95)**
   - recommendation: 50ms â†’ **520ms** ðŸ”´
   - product-catalog: 22ms â†’ **125ms** ðŸŸ¡

2. **CPU Usage**
   - recommendation: 20% â†’ **60%** ðŸŸ¡
   - product-catalog: 15% â†’ **68%** ðŸŸ¡

3. **Request Rate**
   - product-catalog incoming: 95 â†’ **980 req/s** ðŸŸ¡

4. **Error Rate**
   - All services: 0% âœ… (no errors, just slow)

---

### 3. Logs - OpenSearch

#### Query 1: Recommendation Service Errors

**API Call:**
```bash
curl -X POST "http://localhost:9200/logs-*/_search" -H 'Content-Type: application/json' -d '{
  "query": {
    "bool": {
      "must": [
        {"term": {"service.name": "recommendation"}},
        {"match": {"message": "cache"}},
        {"range": {"@timestamp": {"gte": "now-5m"}}}
      ]
    }
  },
  "size": 10,
  "sort": [{"@timestamp": "desc"}]
}'
```

**Expected Logs:**

```json
{
  "@timestamp": "2024-12-11T14:30:05Z",
  "service.name": "recommendation",
  "severity": "ERROR",
  "message": "CacheConnectionException: Connection refused to redis:6379",
  "trace_id": "a1b2c3d4e5f6...",
  "span_id": "x7y8z9..."
}
```

```json
{
  "@timestamp": "2024-12-11T14:30:06Z",
  "service.name": "recommendation",
  "severity": "WARN",
  "message": "Cache unavailable, falling back to direct catalog queries",
  "cache_hit_rate": 0.0
}
```

```json
{
  "@timestamp": "2024-12-11T14:30:10Z",
  "service.name": "recommendation",
  "severity": "INFO",
  "message": "Cache connection attempts failing: 25 consecutive failures"
}
```

---

#### Query 2: Product Catalog High Load

```bash
curl -X POST "http://localhost:9200/logs-*/_search" -H 'Content-Type: application/json' -d '{
  "query": {
    "bool": {
      "must": [
        {"term": {"service.name": "product-catalog"}},
        {"range": {"@timestamp": {"gte": "now-5m"}}}
      ]
    }
  },
  "size": 10
}'
```

**Expected Logs:**

```json
{
  "@timestamp": "2024-12-11T14:31:05Z",
  "service.name": "product-catalog",
  "severity": "WARN",
  "message": "Database connection pool pressure: 45/50 connections in use",
  "db_pool_usage": 0.90
}
```

```json
{
  "@timestamp": "2024-12-11T14:31:10Z",
  "service.name": "product-catalog",
  "severity": "INFO",
  "message": "High request rate detected: 980 req/s (baseline: 95 req/s)",
  "request_rate": 980
}
```

---

### 4. Traces - Jaeger

#### Access Jaeger UI:
```
http://localhost:8080/jaeger/ui
```

#### Search for Traces:

**Query 1: Recommendation Service Slow Traces**

```
Service: recommendation
Min Duration: 500ms
Limit: 20
```

**What You'll See:**

**Before failure:**
```
Trace ID: abc123...
Duration: 52ms
Spans:
  â””â”€ recommendation.GetRecommendations (52ms)
     â”œâ”€ cache.Get (2ms) âœ… Cache hit
     â””â”€ (no product-catalog call)
```

**After failure (T+10s):**
```
Trace ID: def456...
Duration: 525ms  ðŸ”´ 10x SLOWER
Spans:
  â””â”€ recommendation.GetRecommendations (525ms)
     â”œâ”€ cache.Get (2ms) âŒ Cache miss
     â””â”€ product-catalog.ListProducts (498ms)  â¬…ï¸ NEW! (cache bypass)
        â””â”€ postgresql.Query (485ms)
```

**Key Observation:** Trace now shows extra hop to product-catalog (wasn't there before)

---

**Query 2: Frontend Traces (Showing Cascade)**

```
Service: frontend
Operation: GET /product/{id}
Min Duration: 300ms
```

**After failure (T+60s):**
```
Trace ID: ghi789...
Duration: 820ms  ðŸ”´ 4x SLOWER
Spans:
  â””â”€ frontend.GetProduct (820ms)
     â”œâ”€ recommendation.GetRecommendations (525ms)  ðŸ”´ SLOW
     â”‚  â””â”€ product-catalog.ListProducts (498ms)  â¬…ï¸ Bottleneck
     â”‚     â””â”€ postgresql.Query (485ms)
     â”œâ”€ product-catalog.GetProduct (85ms)  ðŸŸ¡ Slower than normal
     â”œâ”€ ad.GetAd (42ms) âœ… Normal
     â””â”€ cart.GetCart (18ms) âœ… Normal
```

**Key Observation:** 
- recommendation span is now the bottleneck
- product-catalog appears twice (recommendation + frontend direct call)
- Overall request 4x slower due to recommendation latency

---

**Query 3: Error Traces (if any)**

```
Service: recommendation
Tags: error:true
```

**Might see:**
```
Trace ID: jkl012...
Status: Error
Error message: "Cache connection timeout"
Spans:
  â””â”€ recommendation.GetRecommendations (ERROR)
     â””â”€ cache.Get (ERROR: Connection timeout after 5s)
```

---

### 5. Full Example: Correlation Across Tools

#### T+30s After Injection:

**Prometheus shows:**
```
recommendation P95 latency: 520ms (was 50ms)
recommendation cache hit rate: 0% (was 94%)
recommendation CPU: 60% (was 20%)
product-catalog request rate: 980 req/s (was 95 req/s)
product-catalog CPU: 68% (was 15%)
```

**Grafana Dashboard shows:**
- ðŸ”´ Red spike in recommendation latency panel
- ðŸŸ¡ Yellow spike in product-catalog CPU panel
- ðŸ“ˆ Request rate graph shows 10x jump

**Logs show:**
```
[ERROR] recommendation: CacheConnectionException: Connection refused
[WARN] recommendation: Cache unavailable, falling back to direct queries
[WARN] product-catalog: High request rate: 980 req/s
[WARN] product-catalog: DB connection pool pressure: 90%
```

**Traces show:**
- Slow traces (500ms+) for recommendation service
- Extra spans: recommendation â†’ product-catalog (not present before)
- product-catalog spans under high load (slower than baseline)

---

## How to Detect This As An AI Agent

### Detection Algorithm:

1. **Primary Signal** (Prometheus):
   ```python
   # Check recommendation latency
   latency = query_prometheus(
       "histogram_quantile(0.95, rate(http_server_duration_bucket{service_name='recommendation'}[5m]))"
   )
   
   if latency > 0.5:  # 500ms threshold
       alert("High latency detected in recommendation service")
   ```

2. **Root Cause Analysis** (Prometheus):
   ```python
   # Check cache hit rate
   cache_hit_rate = query_prometheus("recommendation_cache_hit_rate")
   
   if cache_hit_rate < 0.1:  # Below 10%
       root_cause = "Cache failure or unavailable"
   ```

3. **Downstream Impact** (Prometheus):
   ```python
   # Check product-catalog load
   catalog_rps = query_prometheus(
       "rate(http_server_requests_total{service_name='product-catalog'}[1m])"
   )
   
   if catalog_rps > 500:  # Baseline is ~95
       downstream_impact = "product-catalog receiving excessive traffic"
   ```

4. **Correlate with Logs** (OpenSearch):
   ```python
   logs = query_opensearch({
       "query": {
           "bool": {
               "must": [
                   {"term": {"service.name": "recommendation"}},
                   {"match": {"message": "cache"}}
               ]
           }
       }
   })
   
   if "CacheConnectionException" in logs:
       confirm("Cache connection failure")
   ```

5. **Verify with Traces** (Jaeger):
   ```python
   traces = query_jaeger(service="recommendation", minDuration="500ms")
   
   for trace in traces:
       if has_span(trace, target="product-catalog"):
           # Recommendation is calling catalog (cache bypass)
           confirm("Cache miss forcing direct catalog calls")
   ```

---

## Complete Anomaly Signature: Cache Failure

### Metrics Anomalies:

| Metric | Service | Normal | During Failure | Change |
|--------|---------|--------|----------------|--------|
| P95 Latency | recommendation | 50ms | 520ms | **10x â¬†ï¸** |
| Cache Hit Rate | recommendation | 94% | 0% | **â¬‡ï¸ ZERO** |
| CPU Usage | recommendation | 20% | 60% | **3x â¬†ï¸** |
| Request Rate | product-catalog | 95/s | 980/s | **10x â¬†ï¸** |
| CPU Usage | product-catalog | 15% | 68% | **4.5x â¬†ï¸** |
| P95 Latency | product-catalog | 22ms | 125ms | **5.7x â¬†ï¸** |
| Page Load Time | frontend | 200ms | 800ms | **4x â¬†ï¸** |

### Log Patterns:

**Recommendation Service:**
```
[ERROR] CacheConnectionException: Connection refused to redis:6379
[WARN] Cache unavailable, falling back to direct catalog queries  
[INFO] Cache connection attempts failing: X consecutive failures
[ERROR] Redis pool exhausted
```

**Product Catalog Service:**
```
[WARN] High request rate detected: 980 req/s
[WARN] Database connection pool pressure: 90%
[INFO] Serving 10x normal traffic
```

### Trace Patterns:

**Before (Normal):**
```
frontend (200ms)
  â””â”€ recommendation (45ms)
     â””â”€ cache.Get (2ms) âœ…
```

**After (Cache Failure):**
```
frontend (820ms)  ðŸ”´ 4x slower
  â””â”€ recommendation (525ms)  ðŸ”´ 10x slower
     â”œâ”€ cache.Get (2ms) âŒ Miss
     â””â”€ product-catalog.ListProducts (498ms)  â¬…ï¸ NEW SPAN!
        â””â”€ postgresql.Query (485ms)
```

**Key Trace Indicators:**
- âœ… New span appears: `recommendation â†’ product-catalog`
- âœ… `cache.Get` span present but returns miss
- âœ… Overall trace duration 4-10x longer
- âœ… product-catalog spans under higher load

---

## API Examples: Querying the Data

### Prometheus API

**Check Latency:**
```bash
curl "http://localhost:9090/api/v1/query?query=histogram_quantile(0.95,rate(http_server_duration_bucket{service_name=\"recommendation\"}[5m]))"
```

**Response:**
```json
{
  "status": "success",
  "data": {
    "resultType": "vector",
    "result": [
      {
        "metric": {"service_name": "recommendation"},
        "value": [1702311000, "0.520"]  â¬…ï¸ 520ms
      }
    ]
  }
}
```

---

**Check Cache Hit Rate:**
```bash
curl "http://localhost:9090/api/v1/query?query=recommendation_cache_hit_rate"
```

**Response:**
```json
{
  "status": "success",
  "data": {
    "result": [
      {
        "metric": {"service_name": "recommendation"},
        "value": [1702311000, "0.0"]  â¬…ï¸ 0% hit rate
      }
    ]
  }
}
```

---

### Jaeger API

**Find Slow Traces:**
```bash
curl "http://localhost:16686/api/traces?service=recommendation&minDuration=500ms&limit=10"
```

**Response:**
```json
{
  "data": [
    {
      "traceID": "a1b2c3d4e5f6...",
      "spans": [
        {
          "spanID": "span1",
          "operationName": "recommendation.GetRecommendations",
          "duration": 525000,
          "tags": [
            {"key": "cache.hit", "value": false},
            {"key": "cache.miss.reason", "value": "connection_error"}
          ]
        },
        {
          "spanID": "span2",
          "parentSpanID": "span1",
          "operationName": "product-catalog.ListProducts",
          "duration": 498000,
          "references": [{"refType": "CHILD_OF", "spanID": "span1"}]
        }
      ]
    }
  ]
}
```

**Key Fields:**
- `duration`: 525ms (vs baseline ~50ms)
- `tags.cache.hit`: false
- New child span to `product-catalog`

---

### OpenSearch API

**Search Logs:**
```bash
curl -X POST "http://localhost:9200/logs-*/_search" -H 'Content-Type: application/json' -d '{
  "query": {
    "bool": {
      "must": [
        {"term": {"service.name": "recommendation"}},
        {"match": {"severity": "ERROR"}},
        {"range": {"@timestamp": {"gte": "now-5m"}}}
      ]
    }
  },
  "size": 100,
  "sort": [{"@timestamp": "desc"}]
}'
```

**Response:**
```json
{
  "hits": {
    "total": {"value": 45},
    "hits": [
      {
        "_source": {
          "@timestamp": "2024-12-11T14:30:05.123Z",
          "service.name": "recommendation",
          "severity": "ERROR",
          "body": "CacheConnectionException: Connection refused to redis:6379",
          "trace_id": "a1b2c3d4e5f6...",
          "span_id": "x7y8z9...",
          "attributes": {
            "error.type": "CacheConnectionException",
            "redis.host": "redis",
            "redis.port": 6379
          }
        }
      }
    ]
  }
}
```

---

## Complete Detection Flow

### Step 1: Detect Anomaly (Prometheus)
```python
# Monitor latency
latency = prometheus.query("recommendation P95 latency")
if latency > 500ms:
    incident = create_incident("High latency in recommendation")
```

### Step 2: Identify Root Cause (Prometheus + Logs)
```python
# Check cache metrics
cache_hit_rate = prometheus.query("recommendation_cache_hit_rate")
if cache_hit_rate < 0.1:
    root_cause = "Cache failure"
    
# Confirm with logs
logs = opensearch.search("recommendation", "cache", "ERROR")
if "CacheConnectionException" in logs:
    confirmed = True
```

### Step 3: Assess Impact (Prometheus + Traces)
```python
# Check downstream services
catalog_load = prometheus.query("product-catalog request rate")
if catalog_load > 500:
    downstream_impact = ["product-catalog"]
    
# Verify with traces
traces = jaeger.search(service="recommendation", minDuration="500ms")
for trace in traces:
    if "product-catalog" in trace.spans:
        cascade_confirmed = True
```

### Step 4: Determine Severity
```python
severity = calculate_severity(
    latency=10x,
    services_affected=2,
    user_impact="degraded",
    error_rate=0
)
# Result: SEV-3 (Degraded performance, no errors, limited scope)
```

### Step 5: Remediate
```python
# Check feature flags
flag_status = flagd.get("recommendationCacheFailure")
if flag_status == "enabled":
    flagd.set("recommendationCacheFailure", "off")
    
# Verify recovery
wait(30)
latency = prometheus.query("recommendation P95 latency")
if latency < 100ms:
    incident.resolve("Cache failure flag disabled")
```

---

## Key Metrics to Monitor Per Scenario

### Cache Failure:
- âœ… `recommendation_cache_hit_rate` â†’ 0%
- âœ… `recommendation P95 latency` â†’ 10x
- âœ… `product-catalog request rate` â†’ 10x
- âœ… Traces: new `recommendation â†’ product-catalog` spans

### Payment Failure:
- âœ… `payment error rate` â†’ 50%
- âœ… `checkout error rate` â†’ 50%
- âœ… `http_status_code="500"` â†’ increase
- âœ… Logs: "Payment processing failed"

### Kafka Lag:
- âœ… `kafka_consumer_lag{topic="orders"}` â†’ 1000+
- âœ… `accounting last_processed_time` â†’ 8 min ago
- âœ… `fraud_detection_delay` â†’ 8 min
- âœ… Logs: "Consumer lag detected"

### High CPU:
- âœ… `process_cpu_seconds_total{service="ad"}` â†’ 0.95 (95%)
- âœ… `http_server_duration{service="ad"}` â†’ 10x
- âœ… `http_server_requests_active{service="ad"}` â†’ 100+ (queue)
- âœ… Logs: "Thread pool exhausted"

---

## Pre-built Grafana Dashboards

Access: `http://localhost:8080/grafana`

### 1. **Demo Dashboard**
- Overall system health
- Service latency (all services)
- Request rates
- Error rates

### 2. **Span Metrics Dashboard**
- Trace-derived metrics
- Service dependencies (auto-discovered)
- Operation latencies
- Error traces

### 3. **Service-Specific Dashboards**
- Per-service CPU/Memory/Latency
- Request throughput
- Error rates
- Custom service metrics

---

## Testing Your Detection

### Quick Test:

```bash
# 1. Start monitoring (before failure)
curl "http://localhost:9090/api/v1/query?query=histogram_quantile(0.95,rate(http_server_duration_bucket{service_name=\"recommendation\"}[5m]))" | jq '.data.result[0].value[1]'
# Output: "0.050" (50ms)

# 2. Inject failure
./incidentfox/scripts/trigger-incident.sh cache-failure

# 3. Wait 30 seconds
sleep 30

# 4. Check again
curl "http://localhost:9090/api/v1/query?query=histogram_quantile(0.95,rate(http_server_duration_bucket{service_name=\"recommendation\"}[5m]))" | jq '.data.result[0].value[1]'
# Output: "0.520" (520ms) â¬†ï¸ 10x INCREASE

# 5. Check cache
curl "http://localhost:9090/api/v1/query?query=recommendation_cache_hit_rate" | jq '.data.result[0].value[1]'
# Output: "0.0" (0%) â¬‡ï¸ DROPPED TO ZERO
```

---

## Summary: What Shows Anomalies

### âœ… Metrics (Prometheus):
- Latency increases (10x)
- Cache hit rate drops (0%)
- CPU increases (3x)
- Request rate to downstream increases (10x)

### âœ… Logs (OpenSearch):
- ERROR: "CacheConnectionException"
- WARN: "Cache unavailable, falling back"
- INFO: "High request rate detected"

### âœ… Traces (Jaeger):
- Trace duration 10x longer
- New spans appear (cache bypass)
- Downstream service spans under load

### âœ… Dashboards (Grafana):
- Visual spikes in latency panels
- CPU/Memory increases
- Request rate jumps

**All tools show correlated anomalies at the same time!** ðŸŽ¯

---

## Additional Resources

- **Complete endpoint docs**: `agent-config/endpoints.yaml`
- **Query examples**: `docs/agent-integration.md`
- **All failure scenarios**: `docs/incident-scenarios.md`
- **Cascade analysis**: `docs/cascade-impact-analysis.md`


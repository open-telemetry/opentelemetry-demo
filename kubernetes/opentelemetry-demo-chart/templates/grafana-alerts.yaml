{{- if and .Values.grafana.enabled .Values.grafana.alerting.enabled }}
# Grafana Alert Rules for ADX Observability
# These alerts use KQL queries against Azure Data Explorer
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-alerting-rules
  namespace: {{ .Values.namespace }}
  labels:
    grafana_alert: "1"
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: alerting
data:
  alerting.yaml: |
    apiVersion: 1

    # =================================================================
    # Alert Rules - KQL-based queries against ADX
    # =================================================================
    groups:
      - orgId: 1
        name: ADX Observability Alerts
        folder: ADX Alerts
        interval: 1m
        rules:
          # ---------------------------------------------------------
          # High Error Rate Alert
          # Triggers when error rate exceeds threshold
          # ---------------------------------------------------------
          - uid: adx-high-error-rate
            title: High Error Rate Detected
            condition: error_threshold
            data:
              - refId: error_rate
                relativeTimeRange:
                  from: 300
                  to: 0
                datasourceUid: adx-telemetry
                model:
                  querySource: raw
                  resultFormat: table
                  clusterUri: {{ .Values.adx.clusterUri | quote }}
                  database: {{ .Values.adx.database | quote }}
                  query: |
                    OTelTraces
                    | where Timestamp > ago(5m)
                    | summarize TotalSpans = count(), ErrorSpans = countif(StatusCode == "Error" or StatusCode == "STATUS_CODE_ERROR")
                    | extend ErrorRate = round(100.0 * ErrorSpans / TotalSpans, 2)
                    | project ErrorRate
              - refId: error_threshold
                relativeTimeRange:
                  from: 300
                  to: 0
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: error_rate
                  conditions:
                    - evaluator:
                        type: gt
                        params:
                          - {{ .Values.grafana.alerting.thresholds.errorRatePercent }}
                      operator:
                        type: and
                      reducer:
                        type: last
            noDataState: NoData
            execErrState: Error
            for: 2m
            annotations:
              summary: Error rate is above {{ .Values.grafana.alerting.thresholds.errorRatePercent }}%
              description: |
                The overall error rate has exceeded the threshold.
                Current error rate: {{`{{ $values.error_rate.Value }}`}}%
            labels:
              severity: critical
              category: errors

          # ---------------------------------------------------------
          # High Latency Alert (P95)
          # Triggers when P95 latency exceeds threshold
          # ---------------------------------------------------------
          - uid: adx-high-latency
            title: High P95 Latency Detected
            condition: latency_threshold
            data:
              - refId: p95_latency
                relativeTimeRange:
                  from: 300
                  to: 0
                datasourceUid: adx-telemetry
                model:
                  querySource: raw
                  resultFormat: table
                  clusterUri: {{ .Values.adx.clusterUri | quote }}
                  database: {{ .Values.adx.database | quote }}
                  query: |
                    OTelTraces
                    | where Timestamp > ago(5m)
                    | where ParentSpanId == ""
                    | summarize P95Latency = percentile(Duration / 1000000.0, 95)
                    | project P95Latency
              - refId: latency_threshold
                relativeTimeRange:
                  from: 300
                  to: 0
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: p95_latency
                  conditions:
                    - evaluator:
                        type: gt
                        params:
                          - {{ .Values.grafana.alerting.thresholds.p95LatencyMs }}
                      operator:
                        type: and
                      reducer:
                        type: last
            noDataState: NoData
            execErrState: Error
            for: 3m
            annotations:
              summary: P95 latency is above {{ .Values.grafana.alerting.thresholds.p95LatencyMs }}ms
              description: |
                The P95 latency for root spans has exceeded the threshold.
                Current P95 latency: {{`{{ $values.p95_latency.Value }}`}}ms
            labels:
              severity: warning
              category: latency

          # ---------------------------------------------------------
          # Service Down Alert
          # Triggers when no spans received from a service
          # ---------------------------------------------------------
          - uid: adx-service-down
            title: Service Not Reporting
            condition: no_spans_threshold
            data:
              - refId: service_spans
                relativeTimeRange:
                  from: 300
                  to: 0
                datasourceUid: adx-telemetry
                model:
                  querySource: raw
                  resultFormat: table
                  clusterUri: {{ .Values.adx.clusterUri | quote }}
                  database: {{ .Values.adx.database | quote }}
                  query: |
                    let expected_services = dynamic(["frontend", "cart", "checkout", "payment", "product-catalog", "currency", "shipping", "recommendation", "ad"]);
                    OTelTraces
                    | where Timestamp > ago(5m)
                    | distinct ServiceName
                    | summarize ActiveCount = count()
                    | extend MissingServices = 9 - ActiveCount
                    | project MissingServices
              - refId: no_spans_threshold
                relativeTimeRange:
                  from: 300
                  to: 0
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: service_spans
                  conditions:
                    - evaluator:
                        type: gt
                        params:
                          - 0
                      operator:
                        type: and
                      reducer:
                        type: last
            noDataState: Alerting
            execErrState: Error
            for: 5m
            annotations:
              summary: One or more services are not reporting
              description: |
                Expected services are not sending telemetry data.
                Missing services count: {{`{{ $values.service_spans.Value }}`}}
            labels:
              severity: critical
              category: availability

          # ---------------------------------------------------------
          # Error Logs Spike Alert
          # Triggers when error logs exceed threshold
          # ---------------------------------------------------------
          - uid: adx-error-logs-spike
            title: Error Logs Spike Detected
            condition: error_logs_threshold
            data:
              - refId: error_logs
                relativeTimeRange:
                  from: 300
                  to: 0
                datasourceUid: adx-telemetry
                model:
                  querySource: raw
                  resultFormat: table
                  clusterUri: {{ .Values.adx.clusterUri | quote }}
                  database: {{ .Values.adx.database | quote }}
                  query: |
                    OTelLogs
                    | where Timestamp > ago(5m)
                    | where SeverityText == "ERROR" or SeverityNumber >= 17
                    | summarize ErrorCount = count()
                    | project ErrorCount
              - refId: error_logs_threshold
                relativeTimeRange:
                  from: 300
                  to: 0
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: error_logs
                  conditions:
                    - evaluator:
                        type: gt
                        params:
                          - {{ .Values.grafana.alerting.thresholds.errorLogsCount }}
                      operator:
                        type: and
                      reducer:
                        type: last
            noDataState: NoData
            execErrState: Error
            for: 2m
            annotations:
              summary: High number of error logs detected
              description: |
                Error log count has exceeded the threshold in the last 5 minutes.
                Current error count: {{`{{ $values.error_logs.Value }}`}}
            labels:
              severity: warning
              category: logs
{{- end }}
